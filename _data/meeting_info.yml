- presenter: Miseon Park 
  date: 07.11.2025
  paper: "xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection"
  authors: Ben Cohen et al.
  url: https://arxiv.org/abs/2506.22837
  location: "산학기술관 702-1"
  summary: >
    Summary : xLSTMAD는 multi variate time series anomaly detection 위해 xLSTM 기반 인코더-디코더 구조를 활용한 최초의 모델이다.<br>
    Strength : Forecasting과 reconstruction 구조를 모두 활용하고, VUS-ROC·VUS-PR 등 다양한 metric을 통해 신뢰도 높은 성능을 입증함.<br>
    Weakness :  SoftDTW 도입에도 불구하고 MSE 대비 성능 개선이 크지 않으며, forecasting과 reconstruction을 통합하지 않고 별도로 모델링함.<br>

- presenter: Ingyeom Kim
  date: 07.03.2025
  paper: "Flexible mapping of abstract domains by grid cells via self-supervised extraction and projection of generalized velocity signals"
  authors: Abhiram Iyer et al.
  url: https://openreview.net/forum?id=hocAc3Qit7&referrer=%5Bthe%20profile%20of%20Sugandha%20Sharma%5D(%2Fprofile%3Fid%3D~Sugandha_Sharma1)
  location: "산학기술관 702-1"
  summary: >
    Summary : 해당 논문은 뇌세포 grid cell의 작동방식을 모방하여 물체의 위치 변화와 추상적 변화를 저차원 매핑하는 모델을 다루었다.<br>
    Strength : object의 성질과 위치 변화, 추상적 변화를 같이 매핑하는 기존 모델들과는 달리 위치 및 추상적 변화만 효율적으로 매핑시킨다. 또한 다른 차원축소법보다 더 매핑을 잘 하는 모습을 확인 가능하였다.<br>
    Weakness : 현재는 single object밖에 다루지 못하고 실험 설정에서 논리적 오류가 존재하였다.(다만 오류 피하고 재실험시 문제 없던걸로 밝혀짐)

- presenter: Miseon Park 
  date: 06.26.2025
  paper: "This Time is Different: An Observability Perspective on Time Series Foundation Models"
  authors: Ben Cohen et al.
  url: https://arxiv.org/abs/2505.14766
  location: "산학기술관 702-1"
  summary: >
    Summary : 본 논문에서 제안하는 모델 TOTO는 decoder-only Transformer 구조를 기반으로 한 multivariate time series forecasting 모델로, observability 데이터를 학습에 이용했다.  
    시간 축과 변수 축 모두를 고려한 time-wise 및 variate-wise attention을 적용하며, 데이터의 분포 특성은 SMM과 robust Cauchy loss로 처리하고, patch causal norm을 통해 future data leakage를 방지한다.
    대규모 데이터셋인 BOOM을 공개하였다.<br>
    Strength : time-wise와 var-wise attention을 통해 기존 foundation model의 channel independence 가정과 달리 변수 간 상호작용을 학습할 수 있으며, robust loss와 SMM을 활용해 heavy-tailed 및 nonstationary 데이터에 대해 보다 강건한 성능을 보인다.<br>
    Weakness : calendar 정보나 meta feature와 같은 외부 시간 정보를 활용하지 않아 시간적 패턴 학습이 제한적이며, 모델 성능 향상이 아키텍처 때문인지 데이터셋 특성 때문인지는 명확히 분리되어 검증되지 않아 일반화 가능성에 의문이 남는다.

- presenter: Ingyeom Kim
  date: 05.09.2025
  paper: "Provably Learning Object-Centric Representations"
  authors: Jack Brady et al.
  url: https://arxiv.org/abs/2305.14229
  location: "산학기술관 702-1"
  summary: >
    Summary : 해당 논문들은 Object centric learning을 기존의 실험 기반의 모델들에서 벗어나 이론적으로 고찰합니다. (발표 수업 연습 목적으로 발표 했던 것 재발표)<br>
    Strength : Object centric learning의 성패가 latent의 분포가 아닌 모델이 compositionality와 invertibility를 만족하는지에 달려 있음을 밝혀냈고, 이론에 기반하였기에 더 일반적이고 효율적인 loss 구조와 모델을 만들어 내었습니다.<br>
    Weakness : 완성된 모델에 관한 정보 및 실험이 부족한 모습을 보입니다.(후행논문 Interaction Asymmetry: A General Principle for Learning Composable Abstractions, Brady et al. 에 의해 보강됨).

- presenter: Subin Choi
  date: 03.28.2025
  paper: "Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations"
  authors: Mehdi S. M. Sajjadi et al.
  url: https://arxiv.org/abs/2111.13152
  location: "산학기술관 702-1"
  summary: >
    Summary : 이 논문은 interactive rates으로 few images만 가지고 novel views on 3D scene representation을 infer하기 위해 transformer를 사용하여 확장가능한 latent representation을 학습하고, learned attention mechansim을 활용합니다. <br>
    Strength : 강력한 일반화 능력과 높은 효율성을 갖습니다. explicit locally-conditioned geometry에 의존하지 않고 대신 모델이 전역적으로 추론 가능합니다. 복잡한 장면에 대한 scalabilty, 노이즈가 많은 카메라 poses(poses가 없는 경우 포함), semantic segmentation에서도 뛰어난 성능을 보입니다.<br>
    Weakness : 픽셀 단위에서의 정확한 ray 변환을 학습하는 과정에서 불확실성이 발생하고, L2 loss가 불확실성이 클수록 흐림을 유발하므로 다른 loss function을 적용하는 연구가 필요합니다. transformer기반 모델이라 작은 데이터셋에서의 성능은 기대하기 힘듭니다. sparse input& noisy pose에서 강하지만 가까운 view에서는 geomery 방법보다 성능이 낮다.

- presenter: Ingyeom Kim
  date: 03.21.2025
  paper: "Provably Learning Object-Centric Representations, Towards Object-Centric Learning with General Purpose Architectures"
  authors: Jack Brady et al.
  url: https://arxiv.org/abs/2305.14229
  location: "산학기술관 702-1"
  summary: >
    Summary : 해당 논문들은 Object centric learning을 기존의 실험 기반의 모델들에서 벗어나 이론적으로 고찰하고, 이를 기반으로 더욱 효율적이고 일반화된 Transformer 기반 모델을 개발해냅니다.<br>
    Strength : Object centric learning의 성패가 latent의 분포가 아닌 모델이 compositionality와 invertibility를 만족하는지에 달려 있음을 밝혀냈고, 이론에 기반하였기에 더 일반적이고 효율적인 loss 구조와 모델을 만들어 내었습니다.<br>
    Weakness : 완성된 모델에 관한 정보 및 실험이 부족한 모습을 보입니다.(후행논문 Interaction Asymmetry: A General Principle for Learning Composable Abstractions, Brady et al. 에 의해 보강됨).

- presenter: Miseon Park 
  date: 03.14.2025
  paper: "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts"
  authors: Xiaoming Shi et al.
  url: https://arxiv.org/abs/2409.16040
  location: "산학기술관 702-1"
  summary: >
    Summary: 본 논문은 sparse MoE + decoder-only transformer 구조를 활용한 time series forecasting 모델로, Huber Loss, SwiGLU, RMSNorm, RoPE 등 다양한 기술을 적용하여 일반화된 예측 성능을 향상시켰다. <br>
    Strength: multi-resolution head를 통한 다양한 horizon length 예측 가능. zero-shot 과 in-distribution forecasting 모두 높은 성능을 보임. shared expert 구조를 통해 common knowledge를 유지하면서도 top-k expert를 활용하여 token 별 특징을 효과적으로 반영.<br>
    Weakness: channel independence 구조를 통한 예측으로 uni-variate time series forecasting에 가까움. <br>

- presenter: Seungwon Yu
  date: 03.07.2025
  paper: "Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification"
  authors: Yihe Wang et al.
  url: https://arxiv.org/abs/2405.19363
  location: "산학기술관 702-1"
  summary: >
    Summary : 본 논문은 의료 시계열 데이터의 classification을 위한 multi-granularity embedding 및 mulit granularity 끼리의 정보 교환을 위한 router term 방식을 제안함. 의료 시계열 분류를 위하여 subject independent 방식을 주장하며 multi variate, multi time 의 data를 토큰으로 변환하는 embedding 방식을 사용함..<br>
    Strength : 의료 시계열 데이터(eeg)등의 데이터에서 중요한 부분을 embedding dim으로 변환하기 위해 여러 scale의 embedding을 사용하여 높은 성능을 보임.<br>
    Weakness : granularity의 분할이 자동으로 진행되지 않아 여러가지의 조합을 전부 적용해야 하는 불편함이 존재.

- presenter: Subin Choi
  date: 02.27.2025
  paper: "Learning Transferable Visaul Models From Natural Language Supervision"
  authors: Alec Radford et al.
  url: https://arxiv.org/abs/2103.00020
  location: "산학기술관 702-1"
  summary: >
    Summary : 본 논문은 대규모 (image, text) 쌍 data를 이용하여 멀티모달 embedding의 generality, robustness를 향상시킨 모델입니다. text와 image를 각각의 전용 encoder를 통해 임베딩 한 후, 공통 벡터 공간에서의 정렬을 위해 contrastive learning을 이용합니다.<br>
    Strength : zero-shot transfer를 활용하여 별도의 데이터 없이도 다양한 task 수행이 가능합니다. Zero-shot CLIP 모델이 동등한 정확도의 supervision 모델인 ImageNet보다 훨씬 더 robust합니다.<br>
    Weakness : 기본적인 supervised 모델과 비슷한 수준일 뿐, 최신 supervised SOTA 모델보다는 성능이 낮습니다. 새로운 문장을 생성하는 것이 아니라 주어진 이미지에 대한 가장 적절한 문장을 선택하는 방식으로 작동하기 때문에 사용자가 정답 label 후보를 설정해야 합니다. 

- presenter: Ingyeom Kim
  date: 02.20.2025
  paper: "Object-Centric Learning with Slot Attention"
  authors: Francesco Locatello et al.
  url: https://arxiv.org/abs/2006.15055
  location: "산학기술관 702-1"
  summary: >
    Summary : 해당 논문은 attention module의 능력을 이용해 이미지에서 object들을 분할하는 slot attention model을 설명합니다.<br>
    Strength : 기존 MONet, IODINE, DSPN 모델 등 보다 분할 성능이 좋고 메모리 역시 효율적으로 이용합니다. 현재 해당 분야에서 가장 보편적으로 쓰이는 모델 중 하나입니다. 적은 수의 object로 학습시켜도 많은 object를 분할하는 모델을 만들 수 있습니다. 색상 정보에 의존하지 않기에 흑백 이미지에도 적용 가능합니다.<br>
    Weakness : attention module이 어떤 원리로 분할 성능을 향상시키는지에 대한 정보가 부족합니다.
 
- presenter: Miseon Park
  date: 02.13.2025
  paper: "Unified Training of Universal Time Series Forecasting Transformers, MOIRAI-MOE: EMPOWERING TIME SERIES FOUNDATION MODELS WITH SPARSE MIXTURE OF EXPERTS"
  authors: Woo et al.
  url: https://arxiv.org/abs/2402.02592
  location: "산학기술관 702-1"
  summary: >
    "Unified Training of Universal Time Series Forecasting Transformers" <br>
    Summary : 본 논문은 Transformer masked-encoder 구조와 multi patch size를 통해 다양한 frequency, variate에서 사용 가능한 time series forecasting foundation model을 제안함. <br>
    Strength : in-distribution과 out-distribution 모두 좋은 성능을 보여줌. Multivariate time series dataset을 flatten하여 입력하는 방식을 통해, 다양한 time series dataset에 대한 예측을 가능케함. <br>
    Weakness : patch size의 결정이 heuristic하다는 한계가 있음. transformer 입력 길이의 한계 존재.<br>
    "MOIRAI-MOE: EMPOWERING TIME SERIES FOUNDATION MODELS WITH SPARSE MIXTURE OF EXPERTS" <br>
    Summary : 기존의 moriai를 single patch size (projection layer)와 tranformer decoder-only 변형하여 Mixture of Expert(MoE)를 도입함. <br>
    Strength : time series forecasting task에 moe를 처음 적용했음을 주장. token-level specialization을 통해 기존 moirai의 token size에 대한 heuristic한 의존도를 줄임. 기존 moriai의 모든 size 성능을 뛰어넘는 결과를 보여줌. <br>
    Weakness : Activate 되는 parameter의 수는 감소되었다고 주장하지만, 모델의 전체 용량/크기 증가함. Gating(Routing)이 사전 학습된 모델의 임베딩에 의존한다는 한계가 있음. 

- presenter: Seungwon Yu
  date: 02.07.2025
  paper: "LLM-TS Integrator: Integrating LLM for Enhanced Time Series Modeling"
  authors: Can Chen et al.
  url: https://arxiv.org/pdf/2410.16489
  location: "산학기술관 702-1"
  summary: >
    Summary : 해당 논문은 Time series model time series representation과 LLM model의 pattern recognition capability을 mutual information maximization 방식을 사용하여 효율적으로 병합하는 방식을 제안하였음.
    간단한 time series 의 background information과 statistical information 기반의 template 정보를 LLM의 입력으로 사용하여 text representation을 추출하며 TimesNet의 time series representation에 해당 정보가 최대한 포함되도록 학습함.
    prediction loss와 mutual information loss의 중요도를 조절하는 weighting network를 적용하여 학습의 최적화를 도모함.<br>
    Strength : LLM의 정보를 기반으로 TS model을 학습하여 inference 시 복잡도를 낮추며 LLM의 강한 성능이 포함되도록 함.<br>
    Weakness : text template 에 포함된 정보가 너무 적어 실제로 time series representation에 큰 영향을 미치지 못함.

- presenter: Subin Choi
  date: 01.24.2025
  paper: "MONet: Unsupervised Scene Decomposition and Representation"
  authors: Christopher P. Burgess et al.
  url: https://arxiv.org/abs/1901.11390
  location: "산학기술관 702-1"
  summary: >
    Summary : 해당 논문은 비지도학습을 통해 multi-object가 있는 scene에서 개별 object를 인식하고 표현하는 모델입니다. UNET을 이용한 attention network를 통해 mask를 추출해내고, 이를 VAE로 보완하여 robust한 visual object decomposition을 가능하게 합니다.<br>
    Strength : occlusion이 있어도 객체 추론이 가능하며, object 수가 가변적일 때에도 유연하게 표현이 가능합니다. Scene의 element가 분리되어 표현되므로 서로 다른 요소들이 혼합되지 않습니다. latent variable이 독립적인 특성을 제어합니다.<br>
    Weakness : 자연 이미지나 slot 개수를 초과한 object를 포함한 이미지에서는 decompostion이 어렵습니다.

- presenter: Ingyeom Kim
  date: 01.16.2025
  paper: "Constellation: Learning relational abstractions over objects for compositional imagination"
  authors: James C.R. Whittington et al.
  url: https://arxiv.org/abs/2107.11153
  location: "산학기술관 702-1"
  summary: >
    Summary : 해당 논문은 이미지의 object를 분할한 후, GNN의 구조 학습 능력을 통해 객체 간의 추상적 구조를 학습하는 constellation model을 제안합니다. 해당 모델을 통해 추상적 개념의 텍스트 프롬프트 제어를 기대할 수 있습니다.<br>
    Strength : 기존 vision model들의 약점인 추상적 개념 학습을 효과적으로 해보이는 모델입니다. 이를 통해 학습 완료된 모델로 객체의 배열과 같은 추상적 구조를 잘 제어할 수 있습니다.<br>
    Weakness : 텍스트 input을 multi hot encoding으로 처리하기에 자연어 입력이 제한되어 있습니다. sprite dataset 이외에는 아직 실험이 진행되지 않았습니다.
    
  
- presenter: Miseon Park 
  date: 01.10.2025.
  paper: "Chronos: Learning the Language of Time Series"
  authors: Abdul Fatir Ansari et al.
  url: https://arxiv.org/abs/2403.07815
  location: "산학기술관 702-1"
  summary: > 
    Summary : 해당 논문은 LLM을 통해 Time Series Forecasting을 하는 방법을 제안합니다. 시계열 데이터를 scaling, quantization 하는 방법으로 LLM의 input token으로 사용하고 있습니다.<br>
    Strength : 특정 고정된 분포를 사용하지 않고, categorical distribution을 사용함으로서 좀 더 다양한 예상 분포를 나타낼 수 있습니다. 또한 data augmentation을 사용하여 time series dataset의 양적, 질적 한계를 보완하고 있습니다.<br>
    Weakness : 예측 범위가 제한되어 있기 때문에 strong trend에서는 limitation이 있을 수 있습니다.

- presenter: Seungwon Yu
  date: 01.03.2025
  paper: "Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics"
  authors: Caleb Weinreb et al.
  url: https://www.nature.com/articles/s41592-024-02318-2
  location: "산학기술관 702-1"
  summary: >
    Summary : 해당 논문은 기존 depth 기반 syllable 분해 방법인 moseq의 한계점을 타파하고자 keypoint 기반 moseq인 Keypoint-Moseq를 제안함.
    keypoint의 noise가 syllable 분해에 큰 저해요소로 판단하여 frame 별, sensor noise 의 두가지 noise로 모델링 하였으며 동물의 움직을 동역학적으로 모델링하여 높은 성능의 syllable 분해가 가능하도록 함.
    syllable 분해는 AR-HMM 구조에서 이루어 지며 해당 모델에 선형 동역학 모델과 noise모델을 추가한 Keypoint-Moseq를 완성하였음.<br>
    Strength : Depth 기반 데이터가 필요하지 않고 일반적인 영상 기반으로 쉽게 동물의 행동을 syllable 단위로 구분할 수 있는 강한 방법을 제시하였음.<br>
    Weakness : syllable 단위의 구분은 가능하지만 구분 된 syllable이 정확히 어떤 행동인지 파악이 힘든 단점이 존재. unlabelled data를 기반으로 한 학습 방법이기에 정확한 행동을 labelling 하기가 힘듬.

- presenter: Wooyul Jung
  date: 11.22.2024
  paper: "Predictive Attractor Models"
  authors: Ramy Mounir, Sudeep Sarkar
  url: https://arxiv.org/abs/2410.02430
  location: "산학기술관 702-1"
  summary: >
    summary : Backpropagation이 없고 catastrophic forgetting을 예방하며 다양한 미래 가능성을 추론하는 확률적 모델인 PAM을 제안합니다. PAM의 학습방법을 State Space Model과 variational inference를 기반으로 수학적으로 기술하였습니다. 
    strength : Online learning, predictive coding으로 associative memory를 학습하는 방법은 생물학적으로 타당합니다.
    weakness : 크기가 작고 실험의 종류가 associative memory에 치우져 있습니다. 
    question : 모델의 파라미터를 늘릴 경우 성능의 증가율이 최근 dnn 모델 등과 비교하여 어느 정도인지 궁금합니다. associative memory를 decision making이나 motor control등의 과제에 적용가능 여부가 궁금합니다. 

- presenter: Miseon Park
  date: 11.15.2024
  paper: "Graph Neural Network-Based Anomaly Detection in Multivariate Time Series"
  authors: Ailin Deng, Bryan Hooi
  url: https://arxiv.org/pdf/2106.06947
  location: "산학기술관 702-1"
  summary: >
    summary : multivariate time series anomaly detection을 위해 GDN을 소개하고 있습니다. 현실에서의 이상탐지 데이터는 센서 간 상관관계가 중요한 영향을 미치지만, 주어진 dataset만으로 관계를 파악하기엔 한계가 있습니다. 이 논문에서는 sensor embedding과 gnn을 사용하여 그 관계를 보다 잘 파악하고자 합니다. 
    strength : Graph Attention Network와 Node embedding을 활용하여, variate간 상관관계에 대해 보다 잘 파악하고 있습니다. 또한 각 변수의 영향력을 고려한 anomaly scoring 방식을 제안하여 모델의 성능을 향상시켰습니다. 
    weakness : dataset으로 WADI, SWaT 2가지 만을 사용하고 있고, 이전 년도에 나온 GNN 기반 모델에 대한 비교가 부족합니다. 또한, 실제 dataset과 만들어진 adj matrix 간의 차이를 더 확인이 가능했다면 좋을 것 같습니다.  
    question : sensor embedding 방법에서 randomly init이 아닌, 어떤 변화를 줄 수 있을지 궁금합니다. 또한, 노드 수에 따른 embedding 표현력 변화 등이 궁금합니다. variate간 상관관계 파악에 중점이 된 것 같은데, temporal information을 좀 더 잘 활용할 수 있는 방법이 있을 것 같습니다.  

- presenter: Wooyul Jung
  date: 06.27.2024
  paper: "Energy Transformer"
  authors: Benjamin Hoover et al.
  url: https://arxiv.org/abs/2302.07253
  location: "산학기술관 702-1"
  summary: >
    summary : Hopfield network에서 영향을 받은 Energy function을 제안하고 이를 최소화하는 Energy Transformer layer를 소개합니다.
    strength : 수학적 기반이 잘 정립된 Assosiative memory를 가지는 Hopfield network를 적용하였습니다. 또한 attention mechanism을 변경하여는 Energy attention를 제안합니다. Graph anomaly detection task에서 우수한 성능을 가집니다.
    weakness : Image reconstruction이나 graph classification에서는 다른 transformer 모델들에 비해 다소 성능이 낮습니다.
    question : Hierarchical하고 local한 메모리를 가지는 hopfield model을 개발한다면 energy attention 없이 hopfield network만으로 좋은 결과를 낼 수 있을 것 같습니다. 
              또한 classical hopfield network에서 continous attractor를 구현하고 local한 pattern을 인식하는 방향의 발전이 궁금합니다. 
              Graph node classification 수행에 장점을 가지는 것이 수학이나 논리적 사고에 연관을 가지는 것 같습니다.

- presenter: Miseon Park
  date: 06.20.2024
  paper: "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, TIES-Merging: Resolving Interference When Merging Models, Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch"
  authors: 
  url: 
  location: "산학기술관 702-1"
  summary: >
    1. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time ﻿
    [Summary] 여러개의 모델을 합쳐서 하나의 단일 모델로 만듦에도 성능을 유지 및 향상시키고자 합니다. Fineteunig된 모델들의 파라미터들의 평균을 활용하는 uniform soup과 validation의 값을 활용하는 greedy soup에 대해서 이야기하고 있습니다.
    [Strength] 단순한 방법을 통해서 적은 리소스와 계산량으로 실행이 가능합니다. finetuning한 모델들을 합쳐서 하나의 단일모델로서 좋은 성능을 발휘합니다.
    [Weakness] Uniform soup 같은 경우 성능의 감소가 조금 있었고, greedy soup은 validation set이 필요하다는 한계가 있습니다. 
    [Question] Greedy soup에서는 valiation set accuracy 기준 내림차순으로 모델을 할용합니다. 모델의 순서가 최종 결과에 영향을 주지는 않을까요?

    2. TIES-Merging: Resolving Interference When Merging Models
    [Summary] Finetuning된 모델과 Pretrained된 모델 파라미터간의 차이를 이용하는 방법입니다. 성능 감소의 원인으로 redundant parameter와 sign conflict를 이야기하고 있습니다. Trim, Elect Sign, Merge의 세 단게를 통해서 모델을 좀 더 효과적으로 병합합니다. 
    [Strength] model soup, fisher mean, task arithmetic 등 다양한 방법보다 대부분 좋은 성능을 보여줬습니다. Validation set을 활용할 수 없는 경우에도 좋은 성능을 보여줍니다. 
    [Weakness] 기존의 merging방법에 비해서 구현이 복잡하고, validation set이 없는 경우에도 사용은 가능하지만, 성능은 저하되는 결과를 보입니다. 
    [Question] magnitude만을 통해서 redundant parameter와 influental parameter를 구분할 수 있을까요? 

    3. Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
    [Summary] merging을 실행하기 전에 task vector를 전처리 해주는 방법입니다. p의 확률로 masking, 1/(1-p)로 rescailing하는 방법을 통해서 parameter 간의 중복을 통한 성능 하락을 방지합니다. 
    [Strength] 다양한 병합 방법에 함께 사용됨으로서 최종 결과의 성능을 향상시킬 수 있습니다. 
    [Weakness] DARE의 사용이 무조건적인 성능 향상을 보장하지 않습니다. 때로는 성능의 하락을 가져오기도 합니다.   
    [Question] 무작위 masking 때문에 influential parameter가 제거된다면, 오히려 성능에 악영향을 가져오지 않을까 궁금합니다. 또한, 기존 parameter의 정보를 활용하면서 masking할 수 있는 방법이 있지 않을까 고민이 됩니다. 

- presenter: Wooyul Jung
  date: 05.30.2024
  paper: "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
  authors: Albert Gu, Tri Dao
  url: https://arxiv.org/abs/2312.00752
  location: "산학기술관 702-1"
  summary: >
    summary : Transformer는 finite window의 정보에 의존한다는 것과 O(n^2)의 computational cost를 가진다는 단점이 있습니다. 이 문제를 해결하고자 Structured state space sequence model에 selection mechanism과 hardware-aware algorithm을 추가한 모델을 제안합니다. 
    strength : Mamba는 sequence length에 linear한 cost를 가지면서 transformer model과 비슷한 performance를 가집니다. 
    weakness : 많은 수의 파라미터와 데이터로 훈련을 하지 않아 LLM과 비교한 결과를 보여주지 못했습니다.
    question : Seqence length에 비례하는 Parameter수는 SSM의 취지에 부합하지 못한다고 생각합니다. Delta를 조절하여 정보를 선택하는 개념이 흥미롭습니다.

- presenter: Seungwon Yu
  date: 05.23.2024
  paper: "Higher-Order Explanations of Graph Neural Networks via Relevant Walks"
  authors: Thomas Schnake et al.
  url: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547794
  location: "산학기술관 702-1"
  summary : >
    summary : 해당 논문은 Explainable AI 방법인 LRP를 graph에 적용하여 그래프에서 적용되는 흐름을 Relevance walk 라는 개념을 도입하여 모든 walk에 relevance score를 부여합니다.
    해당 방법을 통해서 그래프에서 network를 통과할 때 발생하는 모든 walk에 대해 LPR를 적용하여 relevance score를 부여하는 방법을 제시하고 있습니다.
    strength : 이전 Purturvation 방식과는 다르게 해당 graph structure 자체를 건드려서 중요도를 구하지 않고 입력의 흐름에 따라 가중치를 부여하여 조금 더 직관적인 해석이 가능합니다.
    weekness : 정랑적 지표로의 비교가 불분명합니다. 저자들이 제안하는 비교 방식으로 GNN-Explainer과 PGExplainer 과의 비교에 대한 설명이 조금 부족합니다.
   
- presenter: Ung Hwang
  date: 05.16.2024
  paper: "Graph Contrastive Learning with Cohesive Subgraph Awareness"
  authors: You et al.
  url: https://arxiv.org/pdf/2401.17580
  location: "산학기술관 702-1"
  summary: >
    GCL(Graph Contrastive Learning) 알고리듬에서 Subgraph structure(k-core, k-truss) 정보를 이용해 중요도를 판별하고 얻은 중요도 정보를 이용해 더 나은 augmentation을 하여 contrastive learning을 수행했을때이 성능 향상을 얻을 수 있도록 개선했습니다. 
    또한 GSN(graph substructure network) 논문에서 사용한 substructure 정보를 이용하는 방법을 현재의 augmentation 방식에 알맞도록 보완하는 방식을 제안하여 성능 향상을 얻도록 하였습니다.  
    Strength : 높은 평균 차수를 가지는 그래프 데이터셋(Ex. Social networks)에서 논문이 제안하는 방식에서 높은 개선 성능을 얻을 수 있었습니다.
    Weakness : k-core, k-truss 정보가 잘 활용되기 어려운(Cohesive함을 상대적으로 찾기 어려운) 낮은 평균 차수의 그래프(ex. 분자 정보 등)에선 논문이 제안하는 방식에서 높은 평균 성능 향상을 확인할 수 없었습니다.
    Question : 노드의 중요도는 꼭 Cohesive해야만 높아지는 걸까요? 그래프의 특성과 성질에 맞게 모델의 예측에 주요하게 적용될 만한 다른 기준들이 있을 것 같습니다.

- presenter: YeongHak Jeong
  date: 05.09.2024
  paper: "DenseHMM: Learning Hidden Markov Models by Learning Dense Representations"
  authors: Sicking et al.
  url: https://arxiv.org/abs/2012.09783
  location: "산학기술관 702-1"
  summary: > 
   [Summary] DenseHMM은 hidden states와 observables의 dense representation을 학습할 수 있도록 수정된 HMM입니다. 이는 HMM의 transition probability를 kernalization 함으로써 구현할 수 있습니다. 
   이러한 수정은 constrain이 없는 gradient 기반 optimization을 가능하게 합니다. 논문에서 두 가지 optimization 방법 - Baum-Welch 알고리즘의 수정된 버전과 Co-occurence 최적화 방법을 제시합니다.
   후자의 방법은 scalable 하며 기존 HMM에 비해 성능이 향상된다는 점을 synthetic data와 biology/medical data을 사용하여 실험적으로 보였습니다. 
   Kernelization의 non-linearity가 표현의 풍부함을 높이는데 중요한 역할을 한다는 것을, HMM의 수학적 이론을 기반으로 설명하고 있습니다.
   [Strength] 이 논문은 Dense Representation의 장점을 HMM을 기반으로 설명했고, Sequential Data를 Modeling할 수 있는 다양한 방법과 함께 integration 했을 때의 가능성이 기대됩니다.
   [Weakness] 다만 Dense Representation 자체가 내부 동작을 해석하기 어렵게 할 수 있고, 그 표현이 만드는 계산 비용도 고려해야 할 것입니다. 그리고 EM optimization한 DenseHMM의 성능이 좋지 않은데, 그래서 co-occurence라는 optimization 방법도 같이 제시한 것인지, 두 optimization이 어떠한 관계가 있는지 파악하는 데 개인적인 어려움이 있었습니다.

- presenter: Miseon Park
  date: 05.02.2024
  paper: "Unified Training of Universal Time Series Forecasting Transformers, Large Language Models Are Zero-Shot Time Series Forecasters, A decoder-only foundation model for time-series forecasting"
  authors: 
  url: 
  location: "산학기술관 702-1"
  summary: >
    1. Unified Training of Universal Time Series Forecasting Transformers
    [Summary] Masked Encoder-based Universal Time Series Forecasting Transformer(MOIRAI). Time Series data에는 variation 수, freqeuncy, 분포 등이 다양하게 존재하기 때문에 예측하기가 까다롭습니다. 이 논문에서는 새로운 transformer 구조와 Patch embedding을  통한 Time Series Forecasting Foundation model을 제안합니다. |
    [Strength] 다양한 time series dataset 모음인 LOTSA를 제안했으며, 이를 통해 다양하고 많은 데이터를 학습시켰다는 장점이 있습니다. 실험 결과를 통해 zero-shot forecasting임에도 full shot model 대비 좋은 성과를 거두었습니다.
    [Weakness] patch size mapping의 구조이기 때문에 때로는 hueristic 하다는 한계가 있습니다.
    2. A Decoder-Only Foundation Model For Time-Series Forecasting
    [Summary] Time-series Foundation Model(TimesFM). patching input 과 decoder only model 구조로 이루어진, time series forecasting model입니다. 예측의 정확도를 높이기 위해 output patch size를 input patch size 보다 크게 예측했습니다. 
    [Strength] synthetic dataset을 사용하여 성능을 향상시켰습니다.
    [Weakness] 최근에 나온 논문임에도 불구하고 최신모델과의 비교가 부족합니다. 
    3. Large Language Models Are Zero-Shot Time Series Forecasting
    [Summary] LLMTime. LLM 모델을 time series forecasting에 사용한 모델입니다. tokenization된 time series 입력을 rescailing하여 zero-shot forecasting을 진행했습니다. 또한, continuos likelihood 방법을 사용하여 예측의 정확도를 향상시켰습니다.
    [Strength] 특별한 fine tuning 없이도 LLM 그 자체만으로도 forecasting이 가능함을 시사했습니다.
    [Weakness] 다른 논문에서도 언급되듯이 성능이 경쟁적이지는 않았다는 한계가 있습니다.
  
- presenter: HyunGeun Lee
  date: 04.18.2024
  paper: "LFADS - Latent Factor Analysis via Dynamical Systems"
  authors: Sussillo et al.
  url: https://arxiv.org/abs/1608.06315
  location: "산학기술관 702-1"
  summary: >

- presenter: Wooyul Jung
  date: 04.11.2024
  paper: "Discrete Graph Structure Learning for Forecasting Multiple Time Series"
  authors: Shang et al.
  url: https://arxiv.org/abs/2101.06861
  location: "산학기술관 702-1"
  summary: >
    summary : 본 논문은 graph structure를 학습하는 multivariate time series forecasting model을 소개합니다. 
      GTS(graph for time series)는 NRI, DCRNN를 포함하는 baselines과 비교해서 time series forecasting 성능에 더 좋은 성능을 가집니다.
      논문은 graph structure의 priori knowledge가 더 나은 forecasting quality를 만드는 것에 도움을 준다고 말합니다. 
    strength : NRI와는 달리 같은 time series에서 같은 graph structure를 생성합니다. DCRNN을 사용함으로써 multivariate time series 데이터를 잘 분석할 수 있습니다. 
    weakness : 상황에 따라 다른 structure를 가지는 것이 더 real world data에 가까울 것입니다.
    question : Dot product를 사용하지 않고 concat을 하는 이유가 궁금합니다. 저희 연구에서 T-GCN이나 DCGRU 대신 일반적인 gru를 사용한 이유가 궁금합니다. 
      Mulivariate Dynamic의 real connectivity를 추론하는 논문의 발전 방향이 궁금합니다. 

- presenter: Seungwon Yu
  date: 04.04.2024
  paper: "Parameterized Explainer for Graph Neural Network"
  authors: Luo et al.
  url: https://arxiv.org/pdf/2011.04573.pdf
  location: "산학기술관 702-1"
  summary : >
    summary : 본 논문은 single instance 를 target으로 하는 GNNExplainer의 성능을 개선하기 위해 mutual information 값의 loss로 직접적인 mask update를 진행하지 않고 mask의 확률값을 binary concrete distribution으로 나타내고 해당 분포의 변수인 latent variable w 의 값을 학습하는 방식을 사용합니다.
    학습된 parameter 각 edge가 존재할 확률값을 구하는데 쓰이며 해당 값은 양쪽 node의 feature representation을 concat한 값을 입력으로 합니다.
    strength : 해당 논문은 GNNExplainer 대비 높은 성능과 훨씬 빠른 속도를 보여줍니다. 각 node 및 각 graph에 대하여 새로 학습을 진행하고 inference를 해야하는 GNNExplainer 대비 연산량의 큰 감소를 보이고 있습니다.
    weekness : 표현되는 subgraph의 크기를 제한하는 buget constraint term과 같은 node를 포함하는 edge를 연결하는 connectivity constraint term 에 대한 설명이 조금 부실합니다. 해당 constraint는 subgraph를 구성하는 중요한 요소임에도 불구하고 너무 간략하게만 설명되어있습니다.

- presenter: Ung Hwang
  date: 03.28.2024
  paper: "Future Directions in Foundations of Graph Machine Learning"
  authors: Morris et al.
  url: https://arxiv.org/pdf/2402.02287.pdf
  location: "산학기술관 702-1"
  summary: >
    본 논문에서는 기존의 1-WL test의 기준에만 focus를 둔 취지의 논문들과 또 그것을 용인하는 듯한 학회들의 반응에 대해 의문을 던지며 본 논문을 작성하였습니다. 제안된 논문들이 현실에서 잘 작동 안되는 이유들이 있는데, 그것들을 표현력, 일반화 능력, 최적화의 문제로 접근하여 주로 레퍼런스를 같이 소개하면서 문제들을 조명합니다.
    Strength: 기존에 발표된 다수의 논문들과 GNN 모델의 성능 측정면에서 다각적인 차원(표현력, 일반화 능력, 최적화) 에서 바라 볼 수 있게 다양한 문제를 제기해줍니다. 그 중에서 그 문제를 해결하기 좋은 접근 방법을 제시한 논문들을 소개해주는 방식으로 어떤 포인트로 문제 해결을 위해 접근할 수 있는지를 알려줍니다.
    기존의 실험된 사례들을 다수 소개하며 거기서 밝혀진 현상들에 대한 사실을 기반으로 문제를 제기합니다. 자체 실험은 없지만 실험된 결과들에 대한 기반으로 논지를 전개하였기에 길지 않은 내용 속에서도 여러 논문들의 실험 결과를 파악 할 수 있는 점이 장점입니다.
    Weakness : 논문 자체에서 새로 실험을 하거나 기발한 방법에 대해 소개하는 것은 아니었고 이러한 시각으로 GNN 논문 작성에 접근해보자는 논문이었기에 독자로 하여금 GNN 모델에 대한 더 깊은 고찰을 한번 더 생각 할 수 있는 부분에서 끝난 것은 장점이기도 하지만 본 논문의 특성상 한계라고 볼 수도 있습니다. 
    Question : 본 논문에서 제기한 문제들에 대해 기존에 내가 알고 있는 잘 제안된 해결방법들을 가졌던 논문들이 있었나요? 
    아니면 이 논문에서 제기한 문제들은 전부 받아들이기 적합한 제안이었을까요? 무리하게 문제제기를 한 부분이 없진 않았을까요?

- presenter: Miseon Park 
  date: 03.14.2024
  paper: "Graph U-Nets"
  authors: Hongyang Gao & Shuiwang Ji
  url: https://arxiv.org/abs/1905.05178
  location: "산학기술관 702-1"
  summary: >
    Summary :  본 논문은 기존 grid data에서 적용되던 U-Nets을 graph에서 적용하고자 하였습니다.   Graph Pooling Layer와 Graph unPooling Layer를 통해 Network embedding을 수행합니다. gPool layer에서는 trainable projection vector p와 k-max pooling을 통해서 sub graph를 생성합니다. unPool layer에서는 만들어진 sub graph를 기반으로 기존의 그래프 형태로 복구시키는 작업을 수행합니다. 
    Strength :  기존의 그래프에서 더 중요한 노드에 가중치를 줄 수 있습니다. 또한 graph connectivity augmentation via graph power 등의 방법을 이용하여 GCN을 더욱 효과적으로 사용하고 있습니다. 
    Weakness : COLLAB datasets의 경우 비교 모델인 DiffPool-DET보다 낮은 성능을 보여주고 있습니다. 또한, 변형된 GCN을 사용하고 있는데, 기존의 GCN을 사용한 결과와의 차이를 보여주지 않습니다.
    Question : 변형된 GCN을 사용하고 있는데, 기존의 GCN을 사용한 결과와 얼마나 유의미한 차이가 있는가 궁금합니다. 또한 저널 클럽 시간에 이야기한 것처럼 edge feature를 이용해서 classification을 하는 방법에 대해서 궁금해졌습니다. 

- presenter: YeongHak Jeong
  date: 03.07.2024
  paper: "Position-aware Graph Neural Networks"
  authors: You et al.
  url: https://arxiv.org/abs/1906.04817
  location: "산학기술관 702-1"
  summary: >
   [Summary] 본 논문은 기존의 GNN이 isomorphic graph의 symmetric한 node를 구별하지 못하는 structure-aware embedding의 한계를 해결하기 위해, node의 position을 함꼐 저장하는 방법을 제안하고 있습니다. 
   Anchor-set을 임의로 지정하여 node의 symmetry를 깨고, Anchor-set과 node 간의 거리 정보를 propagate하고자 하는 message에 포함시킴으로서 position-aware embedding 또한 가능하다고 주장하고 있습니다.
   Anchor-set의 개수를 적절히 정하기 위해 metric space와 관련된 수학 이론을 활용했습니다.
   [Strength] 위와 같은 요약을 기반으로 만들어진 P-GNN은 다양한 dataset과 task에 대하여 GNN에 비해 더 좋은 성능을 보였습니다. 
   특히 position 정보가 이미 dataset에 포함되어 있거나 node 자체가 identifier를 가지고 있는 transductive learning에서는 GNN에 비해 그렇게 큰 성능 개선을 보이지 않았음을 보였습니다. 제안된 framework이 position-aware 하다는 것을 주장하려는 논조로 보였습니다.
   [Weakness] Position을 지정하기 위해 '최단 경로'를 쓰는 것이 모든 graph에 유효할지에 대한 의문이 있습니다. 그리고 isomorphic network 이외의 case에는 범용성있게 쓰일 수 있는 framework인가에 대한 생각이 듭니다.
   Anchor-set sampling, distance-weighted aggreation 연산이 추가되면서, 기존 GNN와 비교하여 계산 복잡도는 어떠한지, 모델이 어떻게 node를 분류했는지에 대해 해석하는데 어려움이 있었습니다.


- presenter: HyunGeun Lee
  date: 02.28.2024
  paper: "What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior"
  authors: Behrens et al.
  url: https://www.sciencedirect.com/science/article/pii/S0896627318308560
  location: "산학기술관 702-1"
  summary: >
    Summary & Strength
    본 논문은 Edward Tolman이 개발한 개념인 "Cognitive map"에 대한 리뷰를 다루고 있습니다. Tolman은 쥐가 먹이를 찾아나가는 실험에서 한 환경에서 먹이를 찾도록 한뒤, 환경을 바뀌어 기존 경로가 막혀있을 때 쥐가 먹이를 찾아나가도록 지름길을 탄다는 것을 발견하게 됩니다. 
    이는 향후 신경과학자들의 여러 연구를 통해 hippocampal-entorhinal system으로 구성된 네트워크로 그 작용이 이루어진다는 것이 밝혀지고 있고, 주로 공간 내에서의 탐색에 특화된 뉴런들로 구성되어 있다는 것을 알게됩니다. 
    이 논문은 이러한 시스템이 공간 이동뿐만 아니라 다른 비공간적인 작업에 대해서도 '구조'와 '예측'이라는 개념으로 접근하여 설명할 수 있는 통합적인 시스템으로 볼 수 있다고 주장하며 관련 토픽에 대하여 리뷰합니다.
    이를 뒷받침하기 위해, 특정 위치에서 발화하는 뉴런인 place cell이 특정 주파수의 소리를 들을 때도 반응하는 예시, 다리와 목이 늘어나는 새의 영상에 규칙적으로 반응하는 GridCell 등을 소개합니다. 
    또한, Medial entorhinal cortex에서는 태스크의 구조적 정보를 처리하고, lateral entorhinal cortex에서는 감각 입력을 처리하고 이를 통합하여 hippocampus에 전달하는 과정인 hippocampal remapping, factorization에 대한 개념과 연구를 소개합니다. 
    더불어, 구조적 추상화와 추론을 위한 중요한 요소로서 인공지능의 일반화와 유연한 행동의 성능 향상에 기여할 것이라 주장합니다. 

- presenter: Seungwon Yu
  date: 02.21.2024
  paper: "GNNExplainer: Generating Explanations for Graph Neural Networks"
  authors: Ying et al.
  url: https://arxiv.org/abs/1903.03894
  location: "산학기술관 702-1"
  summary: >
    summary : 본 논문은 graph를 해석하기 위한 explainable AI 방식을 제안합니다. Graph의 경우 feature 의 정보와는 다르게 Node 간의 연결성이 중요하여 두가지의 정보를 모두 고려하여야 합니다.
    이러한 정보를 고려하여 저자들은 sub graph 형태를 생성하여 원본 graph 형태와 Mutual Information(MI)를 비교하여 가장 높은 값을 가지는 sub graph 의 구조를 찾아갑니다.
    Strength : Graph 형태를 해석하기에 수월한 형태를 처음으로 제안하였으며 효율적으로 그래프 형태의 Explainable Ai 방식을 제안하였습니다.
    Weakness : 현재 진행하고 있는 Pose Estimation에 적용하기가 어려워 보입니다. 해당 방식은 Ground Truth 의 subgraph 를 가지고 학습되는데 현재 사용하는 Data에서는 그러한 정보를 얻기가
    쉽지 않을 것 같습니다. 또한 Node classification 시 각각의 node에 대하여 모든 적용을 해야하여 연산량의 증가가 필수적입니다.
    

  
- presenter: Wooyul Jung
  date: 02.07.2024
  paper: "Graph Neural Networks with Learnable Structural and Positional Representations"
  authors: Dwivedi et al.
  url: https://arxiv.org/abs/2110.07875
  location: "산학기술관 702-1"
  summary: >
      Summary : 이 논문은 structural과 positionial representations를 따로 학습하고자 하는 아이디어를 제시합니다. 제시하는 architecture인 LSPE는 random walk feature를 Positional Encoding initialization
      으로 사용하여 high-order position information을 가집니다. 본 구조는 어떤 sparse GNNs이나 transformer GNNs에서도 적용이 가능하며 성능을 향상시킵니다.   
      Strength : 기존의 positional representation과는 다르게 structure와 따로 학습을 시켜 더 정확하고 추론에 도움이 되는 PE를 생성할 수 있습니다. Laplacian PE와는 다르게 Random Walk PE는 1-WL test를 통과하며 
      sign shifting 현상을 가지지 않습니다.   
      Weakness : positional encoding에 대한 사전 조사나 비교 실험이 부족합니다. PE를 학습하고자 하는 아이디어에 대한 근거를 가지지 않습니다.   
      Question : 과연 positional encoding으로써 효과가 있는 것인지 아니면 multihead적 접근으로써의 연산이 효과가 있는 것일지 궁금합니다. 또한 RWPE 대신에 random initialization을 한다면 결과가 어떻게 달리지는지 
      실험해보고 싶습니다. positional representation을 학습하는 것의 의미를 파악하고자 훈련을 마치고 난 후의 positional feature을 RWPE initial 값와 비교하여 보고 싶습니다.
      
- presenter: Ung Hwang
  date: 01.31.2024
  paper: "Identity-aware Graph Neural Networks"
  authors: You et al.
  url: https://ojs.aaai.org/index.php/AAAI/article/view/17283
  location: "산학기술관 702-1"
  summary: >
    Summary :
    ID-GNN (Identity-aware Graph Neural Network)는 기존 그래프 신경망(GNN)의 표현력 한계를 극복하기 위해 제안된 모델입니다. 
    기존 GNN 모델들이 그래프의 구조적 정보들을 주로 사용하여 노드의 특징을 업데이트하기 때문에, 이러한 모델들은 서로 다른 그래프 구조에서 동일한 이웃 구성을 가진 노드들을
    구별하는 데 어려움을 겪습니다. ID-GNN은 각 노드의 고유 신원(identity) 정보를 ego network의 구성들로 활용해 학습하는 방식으로 모델에 통합함으로써 이러한 문제를 해결하고자 했습니다.
    Strength :
    향상된 표현력: ID-GNN은 ego-network를 통합 노드의 고유 신원 정보를 활용함으로써, 기존 GNN 모델들이 구별하지 못하는 노드들을 효과적으로 구별할 수 있습니다. 이는 모델의 표현력을 크게 향상시킵니다.
    유연성: ID-GNN은 다양한 유형의 그래프 데이터에 적용될 수 있는 유연성을 제공합니다. 이는 신원 정보의 통합 방식이 다양한 그래프 구조와 호환될 수 있기 때문입니다.
    이론적 근거: ID-GNN의 설계는 그래프의 동형이 아님(isomorphism)을 판별할 수 있는 이론적 능력에 근거하고 있어, 모델의 설계 및 분석에 강력한 이론적 근거를 제공합니다.
    Weakness : 
    계산 복잡도: 각 노드에 대해 1부터 K까지의 길이를 가진 사이클의 개수를 계산하고, 이를 노드의 특징 벡터에 추가하는 간단한 방식인 IDGNN-FAST 방법을 추가로 제안했지만 
    ID-GNN 본 내용 자체는 기존 방식 대비 4배가량의 연산속도를 가집니다.
    이는 노드의 신원 정보를 모델에 통합하기 위해 추가적인 계산을 수행해야 한다는 것을 의미하며 본 논문을 직접 적용하고자 하는데 있어 단점이 됩니다.
    Question :
    실험결과에서 ID-GNN은 대개의 경우 기존의 알고리듬의 성능을 상회하는 결과를 보였지만, 약간의 경우에서는 더 낮은 결과를 보이기도 했습니다.
    ID-GNN이 신원 정보를 보다 이전의 방식에 비해 더 정확하게 제공한다면, 이러한 결과는 어떻게 나오는 것일까요? 

- presenter: Miseon Park
  date: 01.24.2024
  paper: "Multivariate Time-series Anomaly Detection via Graph Attention Network"
  authors: Zhao et al.
  url: https://arxiv.org/abs/2009.02040
  location: "산학기술관 702-1"
  summary: >
    본 논문에서는 다변량 시계열 데이터의 이상탐지를 위해 self-supervised 모델인 Multivariate Time series Anomaly Detection via Graph Attention Network(MTAD-GAT)를 제안합니다.
    이 모델은 normalization과 spectral residual을 이용하여 data preprocessing을 진행합니다. 이후 두 개의 병렬 graph attention network를 이용하여 feature dependency와 time dependency를 잘 파악합니다.
    또한 이 모델은 forecasting based model과 reconstruction based model을 모두 사용함으로써, 한 가지 방법만을 사용한 기존의 모델들에 비해 높은 탐지율을 보여줍니다.
    두 모델을 사용해서 계산한 inference score는 peaks over threshold(POT)를 이용하여 이상 또는 정상으로 구분되게 됩니다. 
  
- presenter: YeongHak Jeong
  date: 01.17.2024
  paper: "Variational Graph Auto-Encoders"
  authors: Kipf et al.
  url: https://arxiv.org/abs/1611.07308
  location: "산학기술관 702-1"
  summary: >
    본 논문에서는 Graph Distribution을 어떻게 Variational Auto Encoder로 학습할 것인가에 대한 아이디어를 설명하고 있습니다.
    Graph를 Generate하는 함수를 정확하게 알기 어렵기 때문에, 이를 잘 근사할 수 있는 함수를 이용하여 추론하는 것이 목적입니다.
    그리고 Generative 함수를 만들기 위해, Graph의 Distribution을 low dimension의 latent space에 저장할 필요가 있는데, 이 때 Graph를 Gaussian의 parameter (mean, variance)로 저장한다는 점에도 주목할 필요가 있습니다.
    Link prediction task를 수행한 결과, Cora, Citesser에 대해 Variational Inference 한 경우, 그리고 Cora, Citeseer, Pubmed에 대해 node feature를 함께 학습시킨 경우애 더 좋은 성능을 보인다는 결과를 도출했습니다.
    더 나은 모델 생성과 학습을 위해, 기존의 Gaussian보다 better suit한 prior distribution을 찾아보아야 할 필요가 있다고 주장하고 있습니다.
  
- presenter: HyunGeun Lee
  date: 01.10.2024
  paper: "AMAG: Additive, Multiplicative and Adaptive Graph Neural Network For Forecasting Neuron Activity"
  authors: Li et al.
  url: https://openreview.net/forum?id=7ntI4kcoqG
  location: "산학기술관 702-1"
  summary: >
    Summary & Strengths:
    이 논문은 신경 반응에 대한 forecasting task를 통해 Neural population dynamics를 모델링하는 것을 다룹니다. 
    저자들은 해당 태스크에서 adjacency matrix를 random initialization할 경우 학습과정이 불안정해진다고 주장하며, correlation matrix의 형태의 prior를 도입하며 성능을 향상했습니다.
    또한 Additive, Modulator Message Passing과 더불어 샘플별로 적응할 수 있는 Adaptor를 도입한 AMAG라는 그래프 신경망을 개발하여 Synthetic data와 Monkey data에서의 높은 성능을 보임을 서술하였습니다. 
    Weaknesses: 
    Correlation matrix를 도입함으로써 성능이 향상된다는 것을 empirical analysis를 통해 보였는데 이에 대한 이론적인 배경이 추가된다면 더욱 좋을 것 같다는 생각이 들었습니다.
    [1]번 논문에 서술된 바와 같이 Neurons의 반응 패턴에 correlation이 존재한다는 것은 direct connection을 의미하기도 하지만, Correlation이 높은 두 뉴런에 동일한 input이 들어오는 것을 의미하기도 합니다.
    즉, 이러한 ‘explain away’ correlation이 존재하는 뉴런들 사이의 연결관계 또한 AMAG 모델이 설명할 수 있는 것인지 의문입니다.
    [1] Systematic errors in connectivity inferred from activity in strongly recurrent networks

- presenter: Wooyul Jung
  date: 01.03.2024
  paper: "Neural Relational Inference for Interacting Systems"
  authors: Kipf et al.
  url: https://arxiv.org/abs/1802.04687
  location: "산학기술관 702-1"
  summary: >
    본 논문은 Interacting system에서 dynamic model을 훈련하며 relational structure를 동시에 추론하는 방법인 NRI를 제안하였습니다. 
    Neural relational inference (NRI) model은 discrete latent graph를 통해 GNN으로 dynamics를 학습하고 추론된 edge type으로 interaction을 분류하게 됩니다.
    본 논문에서는 Physics simulation task를 집중적으로 다뤘습니다. 다른 연구와는 달리 explicit interaction structure를 사용하여 dynamic model의 interaction을 학습합니다.
    Latent graph structure를 추론하기 위해 VAE를 사용하였고, 물리현상에 대해서는 Markovian assumption를 적용하여 dymanic을 예측한다는 특징을 가집니다.
    Real-world의 많은 예시에서 interaction이 dynamic하나 본 모델은 static interaction graph만을 이해할 수 있다는 한계점을 가집니다.
    
- presenter: Seungwon Yu
  date: 12.27.2023
  paper: "Graph Transformer Networks"
  authors: Yun et al.
  url: https://arxiv.org/abs/1911.06455
  location: "산학기술관 702-1"
  summary: >
    summary:
    본 논문은 기존의 대부분의 GNN model 이 graph를 fixed & homogenous graph로 생각하고 해석하는데 한계점을 제시하며 다양한 edge와 node type을 가진 heterogenous graph 로 해석하는 방법을 제시합니다.
    또한 기존의 heterogeneous graph 해석 논문에서 제시하는 고정된 meta-path 방법의 문제점을 제시하고 meta-path 자체를 추정하는 Network를 제안합니다.
    Adjacency matrix 의 곱으로 연결되지 않은 node 끼리의 meta-path 생성이 가능하다는 점을 이용하여 1D-conv 를 사용하여 Selected Adjacency Matrices 를 생성하게 됩니다.
    해당 1D-conv는 softmax 함수를 적용하여 meta-path 생성 시 adjancy matrix에 대한 attention score 역할을 하게 됩니다.
    이후 생성된 meta-path 들에 전부 GCN, GAT 를 적용하고 해당 정보를 concat 하여 최종 downstream task를 진행합니다.
    strength : 
    본 논문은 기존의 사람의 지식에 기반하여 pre-define 된 meta-path 가 실제 gnn network 가 참조하는 metapath와 차이가 있다는 점을 밝혀내고 그래프의 정보에 따라 meta-path를 fit하게 생성이 가능합니다
    또한 전체 알고리즘이 아닌 meta-path 생성 알고리즘을 제안하여 해당 meta-path를 기반으로 기존의 고성능 GNN 알고리즘을 함께 사용할 수 있습니다
    weakness : 
    생성된 metapath 전체에 대해서 GNN 알고리즘을 적용해야 하기 때문에 연산량이 높아지며 해당 논문에서도 2개의 Selected Adjacency Matrices 에 대한 결과만 보여주고 있습니다.
    
    
    
  
- presenter: ""
  date: 12.2023
  paper: ML4H & NeurIPS Conference
  authors: "None"
  url: "None"
  location: "New Orleans"
  summary:
  
- presenter: HyunGeun Lee
  date: 11.30.2023
  paper: "Modular meta-learning"
  authors: Alet et al. 
  url: https://arxiv.org/abs/1806.10166
  location: "산학기술관 702-1"
  summary: > 
    본 논문에서는 다양한 방식으로 결합할 수 있는 신경망 모듈 세트를 학습하기 위한 전략을 소개합니다. 
    서로 다른 관련 태스크에 대한 훈련을 통해 이러한 모듈을 재사용 및 결합하여 새로운 태스크에 적용하여 해결할 수 있으며, 이를 통해 더 나은 예측 성능을 얻을 수 있음을 확인할 수 있습니다. 
    저자들은 두 가지 로봇공학 관련 문제에서 접근 방식의 효과를 입증하였습니다.

- presenter: YeongHak Jeong
  date: 11.23.2023
  paper: "Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps"
  authors: George et al. 
  url: https://www.nature.com/articles/s41467-021-22559-5
  location: "산학기술관 702-1"
  summary: >
    본 논문은, human brain이 spatial & conceptual 정보를 어떻게 cognition 할 수 있을까에 대한 motivation에서 시작되었습니다.
    여기서 이 cognition을 표현하기 위해 'Cognitive map'을 만들고자 하는데, 사람이 실제로 cognition을 할 때는 어떠한 관찰이 다양한 맥락에서 이루어지는 aliased 특성을 가지기 때문에 - 이 현상을 모두 대표할 수 있는 model을 만드는 것은 어렵고, 수많은 시도 또한 이루어져 왔습니다.
    이 논문에서는 Cloned-structured cognitive graph (CSCG)를 제안하여, 기존의 이론을 보완할 수 있는 cognitive map 표현 방식을 제안했습니다. 여기서 표현 방식이란, cognitive map을 cloned hidden markov model에 기반한 probablistic sequential model로 만들고자 한다는 것입니다. 
    cloned hidden markov model에, action을 augmented 한 것을 CSCG로 정의하였고, CSCG를 구성하는 parameters를 optimize하기 위한 이론으로 Expecation-Maximization (EM) Algorithm을 제시했습니다.
    CSCG는 관찰(observation)과 관련된 맥락(context)을 latent space에 저장할 수 있고, 이러한 관찰과 맥락의 연결성이 Graph의 형태를 띕니다.
    CSCG을 python으로 구현하여, 기존의 cognition과 관련된 neuroscience의 연구 결과를 이를 이용하여 재현하였고, CSCG가 neuroscience에서 밝혀낸 결과를 잘 설명하고 있다고 주장하고 있습니다.

- presenter: Miseon Park
  date: 11.21.2023
  paper: "An Efficient Graph Convolutional Network Technique for the Traveling Salesman Problem"
  authors: Joshi et al. 
  url: https://arxiv.org/abs/1906.01227
  location: "산학기술관 702-1"
  summary: >
    본 논문은 TSP 문제 해결을 위해 새로운 non-autoregressive learning-based 방법을 제안합니다. 
    먼저, Graph Convolutional Network를 사용하여 그래프의 표현을 heat map으로 나타냅니다.
    그 다음 만들어진 heat map을 Beam Search를 통해 non-autoregressive하게 경로를 찾아냅니다.  
    이러한 방법은 고정된 크기의 그래프인 경우 해결 품질, 추론 속도, 샘플 효율성 측면에서 기존의 autoregressive deep learning model을 능가했습니다.
    그러나, 표준 Operation Research solver에는 미치지 못했습니다. 
  
- presenter: Seungwon Yu
  date: 11.16.2023
  paper: "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition"
  authors: Yan et al. 
  url: https://arxiv.org/abs/1801.07455
  location: "산학기술관 702-1"
  summary: > 
    summary :
    본 논문은 graph-based 로 video action recognition을 진행하는 방법을 제안합니다.
    RGB 형태의 video를 openpose 알고리즘을 사용하여 2D skeleton으로 변환한 후 skeleton의 형태를 graph로 해석하여 각 feature 간의 update를 진행합니다.
    Update 시 1d convolution을 사용하여 다른 timestamp의 같은 joint 의 정보를 message-passing 한 후 spatial data의 정보를 gcn을 통하여 update 하는 방식을 사용합니다.
    strength :
    본 논문에서 저자들은 2D skeleton 및 3D skeleton data를 전부 사용하였는데 이는 2D, 3D data의 입력 pixel 위치만을 참고하여 해석을 진행하기 때문에 가능하며 2D, 3D 형태를 전부 적용이 가능하다는 장점을 가집니다.
    실제 실험 시 CNN Based 방법보다 연산량이 월등하게 작아 학습 및 추론 속도에서의 강점을 가집니다.
    weakness : 
    전체 timestamp를 참조하여 action recognition을 진행하기 때문에 여러 영상이 함께 나오는 task에서는 사용하기 힘들 수 있습니다. sliding window 방식을 접목하면 해당 문제점을 해결할 수 있을수도 있습니다.

- presenter: HyunGeun Lee
  date: 11.09.2023
  paper: "Graph Element Networks: adaptive, structured computation and memory"
  authors: Alet et al. 
  url: https://arxiv.org/abs/1904.09019
  location: "산학기술관 702-1"
  summary: > 
    Graph Element Networks는 partial differential equation의 solver 중 하나인 Finite element method에서 아이디어를 얻은 모델로서, function은 locally linear하다는 inductive bias를 가지고 있습니다. 
    저자는 이러한 특성을 바탕으로 function to function mapping task에 GENs 모델을 적용하였습니다. 
    GENs 모델은 기존의 그래프의 데이터 포인트를 직접적으로 GNN의 node 혹은 edge entities에 상응시켜서 사용하는 것이 아닌 metric space 상에 정의된 data points를 무게중심의 일반화된 방법론과 softmax를 활용한 attention과 흡사한 방식을 통해 grid라는 일종의 template graph의 initial node representation으로 맵핑을 합니다.
    이러한 특성 덕분에 데이터가 그래프 set에 국한되는 것이 아니라 metric space 상에 자유롭게 위치할 수 있게 되고, template grid graph의 density를 조정함으로써 computaional cost 와 accuracy 사이의 trade-off에 있어 유연성을 가진다고 볼 수 있습니다. 
    이 논문에 대하여 한 가지 의문이 드는 점은 GENs의 output이 grid graph의 위치에 대한 미분이 가능하여 template graph의 topology를 조정할 수 있다는 부분입니다. 
    이러한 topology는 철저히 data-driven으로 학습이 되는 것이기 때문에 특정 영역으로 데이터 쏠림이 발생한다면 template graph의 노드 또한 그 쪽으로 모이는 현상이 발생하게 됩니다. 
    이렇게 되면 추후 extrapolation test 혹은 OOD generalization에 있어서 이 현상이 단점으로 부각될 수도 있지 않을까하는 우려가 있습니다.
    이에 대한 검증을 위한 추가 실험이 있었다면 더욱 좋았을 것 같습니다. 

- presenter: Wooyul Jung
  date: 11.02.2023
  paper: "Principal Neighbourhood Aggregation for Graph Nets"
  authors: Corso et al. 
  url: https://arxiv.org/abs/2004.05718
  location: "산학기술관 702-1"
  summary: > 
      위 논문은 'How powerful are Graph Neural Networks?'애 이어서 그래프의 isomorphism problem을 해결하고자 하는 시도입니다.
      discrete feature에서는 sum aggregator가 유용하다는 'How powerful are Graph Neural Networks?'의 주장과는 달리 본 논문은
      continuous feature에 대해서는 multiple aggregators가 필요하다고 말합니다.
      이 주장에 대한 근거로 여러 수학적 접근으로 엄밀성을 갖추었으며 real world bentchmarks에서 기존 방법론보다 훌륭한 결과를 보임을 밝힙니다. 

- presenter: Woong Hwang
  date: 10.12.2023
  paper: "How Powerful are Graph Neural Networks?"
  authors: Xu et al. 
  url: https://arxiv.org/abs/1810.00826
  location: "산학기술관 702-1"
  summary: > 
      저자들은 Graph Neural Network의 aggregation mechanism의 expressive power에 대한 연구를 진행하였습니다. 
      그리고 이를 Graph isomorphism test를 위한 1-WL(Weisfeiler-Lehman) test와 연결지어 설명하며,
      1-hop neighbourhood의 information을 aggregation하는 spatial GNN은 이 1-WL test보다 더 좋은 성능을 낼 수 없다는 것을 보입니다.
      또한, discrete한 feature를 가지는 그래프의 message passing과정을 tree형태로 inject하여 해석하면서 non-isomorphic 그래프 쌍의 aggregation에 대한 구분에 있어 실패하는 사례를 보여줍니다.
      저자들은 이러한 한계점을 극복하기 위해 Sum aggregation의 중요성을 언급하며, Expressive power를 극대화한 GIN 아키텍쳐를 제안하며 graph-level task에서 우수한 성능을 보여주었습니다. 
  

- presenter: HyunGeun Lee
  date: 10.05.2023
  paper: "Neural Message Passing for Quantum Chemistry"
  authors: Gilmer et al. 
  url: https://arxiv.org/abs/1704.01212
  location: "산학기술관 702-1"
  summary: > 
    본 논문은 기존의 다양한 GNN architectures를 하나의 framework로 통합했다는 것에 의미가 큰 논문입니다. 그 framework는 Message Passing Neural Network framework라고 부르며, 이는 총 3가지 단계, Message passing, Update, 그리고 Readout로 구성되어 있습니다.
    먼저, Message Passing 단계에서는 인접 노드의 정보를 계산하고 그 정보를 sum, mean, max, min 등의 연산을 통해 aggregation합니다. 이 때, aggregation된 정보를 message라고 합니다.
    다음으로, 업데이트 단계에서 업데이트 함수 $U_t$는 target node와 메시지의 hidden state를 입력으로 받아 target node의 hidden state를 업데이트합니다. 
    마지막으로, readout 단계에서는 전체 그래프에 대한 representation vector를 계산합니다.
    저자들은 이 Framework를 통해 MPNN Variant를 개발하여 QM9 데이터셋에 대하여 Chemical Accuracy를 달성하였으며, 이 결과에 대한 더 자세한 내용은 논문에 기술되어 있습니다.
    

- presenter: Seungwon Yu
  date: 9.21.2023
  paper: "Graph Attention Networks"
  authors: Veličković et al. 
  url: https://arxiv.org/abs/1710.10903
  location: "산학기술관 702-1"
  summary: > 
    Graph attention networks(GAT)는 2018년도 ICLR에서 발표된 논문입니다. GAT는 마스킹된 self-attention layer를 활용하여 기존의 spectral graph convolution의 단점을 해결했다고 합니다.
    인접한 노드에 정규화된 attention coefficient를 곱하여 합산하는 aggregation 방법론을 사용함으로써, 행렬 연산 및 그래프 구조에 의존하지 않는 방법론을 제안합니다. 
    GAT는 그래프의 구조에 영향을 받지 않기 때문에 기존의 transductive task 뿐만 아니라 inductive task에서도 좋은 성능을 보여준다고 합니다. 
    이를 검증하기 위해 저자는 transductive task 벤치마킹을 위해 Cora, Citeseer 그리고 Pubmed 데이터셋을 사용하였습니다. 
    또한, inductive task에 대한 벤치마킹을 위해 PPI(Protein-protein interaction) 데이터셋을 사용하여 GAT의 SOTA 성능을 검증하였습니다.
    이번 미팅을 통해 GAT에서 사용한 self-attention layer의 수식에 대해 더 깊은 이해를 하였고, GNN 도메인에서 중요하게 여겨지는 방법론에 대한 기초를 다지는 시간을 가졌습니다.

- presenter: Wooyul Jung
  date: 9.14.2023
  paper: "Semi-Supervised Classification with Graph Convolutional Networks"
  authors: Kipf & Welling
  url: https://arxiv.org/abs/1609.02907
  location: "산학기술관 702-1"
  summary: > 
    이번 논문은 spectral graph convolution의 first-order approximation으로 유도되는 graph convolutional network를 제안합니다.
    그동안의 graph based semi-supervised learning에서 graph의 smoothness만 고려한 것과는 달리 위 모델은 edge weight과 node간의 전달을 통해 고차원적인 정보를 다룰 수 있습니다.
    다양한 citation network datasets에서 실험한 결과 다른 모델들에 비해 모두 뛰어난 성능을 보였습니다. 
    발표에서는 수식의 이해를 돕기 위해 graph laplacian과 graph fourier transform에 대한 설명을 더하였으며 마지막에는 transformer와 GCN의 유사성을 다뤘습니다.

- presenter: Hyungeun Lee
  date: 7.19.2023
  paper: "Variance-invariance-covariance regularization for self-supervised learning"
  authors: Bardes et al.
  url: https://arxiv.org/abs/2105.04906
  location: "산학기술관 702-1"
  
- presenter: Hyungeun Lee
  date: 7.19.2023
  paper: "Unsupervised learning of visual features by contrasting cluster assignments."
  authors: Caron et al.
  url: https://arxiv.org/abs/2006.09882
  location: "산학기술관 702-1"
  
- presenter: Seungwon Yu
  date: 7.19.2023
  paper: "Emerging Properties in Self-Supervised Vision Transformers"
  authors: Caron et al.
  url: https://arxiv.org/abs/2104.14294
  location: "산학기술관 702-1"
  
- presenter: Seungwon Yu
  date: 7.19.2023
  paper: "Exploring Simple Siamese Representation Learning"
  authors: Chen et al.
  url: https://arxiv.org/abs/2011.10566
  location: "산학기술관 702-1"
  
- presenter: Seungwon Yu
  date: 7.19.2023
  paper: "Bootstrap your own latent: A new approach to self-supervised Learning"
  authors: Grill et al.
  url: https://arxiv.org/abs/2006.07733
  location: "산학기술관 702-1"
  
- presenter: Woong Hwang
  date: 7.19.2023
  paper: "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations"
  authors: Dwibedi et al.
  url: https://arxiv.org/abs/2104.14548
  location: "산학기술관 702-1"
  
- presenter: Woong Hwang
  date: 7.19.2023
  paper: "A Simple Framework for Contrastive Learning of Visual Representations"
  authors: Chen et al.
  url: https://arxiv.org/abs/2002.05709
  location: "산학기술관 702-1"

- presenter: Taehoon Park
  date: 10.15.2021
  paper: "Learning Neural Causal Models from Unknown Interventions"
  authors: Ke et al.
  url: https://arxiv.org/abs/1910.01075
  pdf: /GraphML-JC/pdf/Learning_Neural_Causal_Models_from_Unknown_Interventions.pdf
  #video: https://youtu.be/fCqoPeP5WRU
  location: "Zoom meetings"
  summary: >
   이번 논문은 system을 contional probability로 modeling 하는 경우에서 graph sturcture recovery에 관한 내용입니다. 
   사용된 graph의 structure는 DAG(Directed Acyclic Graph)입니다. 
   Modeling을 할 때 기존의 system에서 관측가능한 data인 observational data 뿐만 아니라, system에서 일어날 것 같지 않은 
   condition의 값을 할당한 data인 interventional data까지 training 단계에서 사용할 경우 sturcture recovery가 더 좋은 결과를 낸다는 것이 주요내용입니다. 
   또한 모든 실험에서 Intervention을 apply할 때 sparse하게 하나의 node에만 intervention을 사용했습니다. 
   Intervention을 사용할 경우 논문의 모든 dataset(synthetic/real)에서 best performance를 보였으며, 
   어떠한 node에서 intervention이 일어났는 지 모르는 경우, 이를 inference하는 방법도 제시하였으며 괜찮은 성능을 가집니다. 
   Intervention의 경우 data의 perturbation과 동일한 역할을 하는것으로 보여집니다.

- presenter: Donghee Kang
  date: 10.22.2021
  paper: "Contrastive Self-supervised Learning for Graph Classification"
  authors: Zeng et al.
  url: https://arxiv.org/abs/2009.05923
  pdf: /GraphML-JC/pdf/Contrastive_self-supervised_Learning_for_GC.pdf
  #video: https://youtu.be/-VD3UeHTqkU
  location: "Zoom meetings"
  summary: "contrastive self supervised learning 방법을 이용해 graph classification이 발생할 수 있는 overfitting을 예방하려는 방법을 제안하였습니다. augmentation된 그래프들이 같은 그래프에서 나왓는지 아닌지를 prediction하는 모델을 학습한 후 fine-tuning을 통해 classification을 수행하는 방법과 prediction과 classification을 동시에 수행 하며 regularization을 동시에 수행하는 두가지 방법이 있습니다. 두 방법 모두 단순히 classification만을 수행하는 모델보다 좋은 성능을 보였고 overfitting문제를 더 잘 해결 할 수 있었습니다."

- presenter: Hyunmok Park
  date: 10.29.2021
  paper: "Scaling Graph-based Deep Learning models to larger networks"
  authors: Ferriol-Galmés et al.
  url: https://arxiv.org/abs/2110.01261
  pdf:
  #video: https://youtu.be/vAbpM1Hp-vQ
  location: "Zoom meetings"
  summary: |
    Graph Neural Networks (GNN) have shown a strong potential to be integrated into commercial products for network control and management. However, the Graph Neural Networking challenge 2021 brings a practical limitation of existing GNN-based solutions for networking: the lack of generalization to larger networks. The goal of this challenge is to create a scalable Network Digital Twin (i.e., a network model) based on neural networks, which can accurately estimate QoS performance metrics given a network state snapshot. The proposed GNN-Based solution pursues two main objectives:
    
    (1) Finding a good representation for the network components supported by the model.
    
    (2) Exploit scale-independent features of networks.
    
    There are two main properties for networks become larger: (i) higher link capacities and (ii) longer paths. By defining link capacities as the product of a virtual reference link capacity and a scale factor, GNN can learn to make accurate estimates on any combination of these two features. By defining Lmax as a configurable parameter and split flows exceeding Lmax into different queue-link sequences, the model can independently learn each representation. The model was trained on graphs size of |V| = 25 ~ 300 and tested with graphs size of |V| = 50 ~ 300. To generate a wide set of topologies of variable size, all graph topologies have been artificially generate using Power-Law Out-Degree Algorithm. By using grid search data augmentation, the model was trained to cover a broad combination of virtual reference link capacity and a scale factor. As a result, the model obtains better accuracy in topologies seen during the training phase (50 to 99 nodes), achieving an average error of 4.5%. As the topology size increases, the average error stabilizes to ≈10%.

    This paper have presented a novel GNN-based model that addresses a main limitation of existing ML-based models applied to networks: generalizing accurately to considerably larger networks.
  
- presenter: Juhyeon Kim
  date: 11.05.2021
  paper: "Connecting the dots: Multivariate time series forecasting with graph neural networks"
  authors: Wu et al.
  url: https://arxiv.org/abs/2005.11650
  pdf:
  #video: https://youtu.be/q253JT_R8aI
  location: "Zoom meetings"
  summary: |
    Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields. 
    Until now, modeling multivariate time series forecasting they assume that each variable depends on its historical values. 
    But in real world data each variable depends not only on its historical values but also on other variables. And existing methods fail to fully exploit latent spatial dependencies between pairs of variables.
    This paper propose “MTGNN” that can simultaneously learn the graph structure and the GNN for time series in an end-to-end framework.
    The Main frameworks of “MTGNN” are

    1.	Graph Learning Layer
    -	Extract sparse graph adjacency matrix

    2.	Graph Convolution module
    -	Address spatial dependencies among Variables
    
    3.	Temporal Convolution module
    -	Capture temporal patterns
    
    MTGNN achieves great performance on two traffic dataset and show that learning dependencies between variables are more capable of indicating extreme traffic conditions of the central node in advance.

- presenter: ""
  date: 11.12.2021
  paper: "AISTATS Review"
  authors: "None"
  url: "None"
  pdf:
  video: "None"
  location: ""
  summary:
  
- presenter: Haesung Pyeon
  date: 11.19.2021
  paper: "Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks"
  authors: Csordás et al.
  url: https://arxiv.org/abs/2010.02066
  pdf:
  #video: "https://youtu.be/gPpifJ5ulVs"
  location: "Zoom meetings"
  summary: |
    Neural Networks whose subnetworks implement reusable functions are expected to offer numerous advantages. Modularity provides a way of achieving compositionality which is essential for systemic generalization. 
    This paper defines functional modules whether modules implementing specific functionality emerge. 
    By training probabilistic, binary but differentiable masks for all weight, the result shows the module necessary to perform target function. 
    This paper investigates whether NN uses different modules for very different functions and reuses the same modules for the same functions. 
    Many typical NNs are good at specializing modules well but not reusing modules for identical functions. FNN and LSTM share weights depending more on the location of the inputs/outputs than similarity of the performed operations. 
    CNN mostly depend on class-exclusive, non-shared weights in feature detection. Reusing property does not emerge naturally and the same functionality is re-learned (redundant and potentially harmful). 
    There are several potential explanations about poor generalization: NN find it hard to learn to represent data similarly along different information routes that can in principle be processed by a single module. 
    NN might not have learned the correct algorithm to solve some problems. 

- presenter: Taehoon Park
  date: 11.26.2021
  paper: "Perturbation theory approach to study the latent space degeneracy of Variational Autoencoders"
  authors: Andrés-Terré & Lió
  url: https://arxiv.org/abs/1907.05267
  pdf:
  #video: "https://youtu.be/jd60ajpFUmg"
  location: "Zoom meetings"
  summary: |
    VAEs provide a lower dimensional representation of the data, in the form of a set of generative functions capable to reproduce and reconstruct the original input.
    Inspired by perturbation analysis in quantum physics, we have developed an approach to unveil structures and the energy spectrum encoded in the data by using the generative functions extracted from a VAE
    Applying a certain perturbed Hamiltonian over the generated function.

    For implementation, encoder returns the function that returns unique solution for the system. To generate different energy level, perturbation approach linearly combines unperturbed energy and Hamiltonian.

    The embeddings generated from the fully trained VAE provide a space where the clusters are clearly seperable. This work is very beginning of the line of study, and further studies will promising for understanding dynamic system with new approach.

- presenter: Donghee Kang
  date: 12.03.2021
  paper: "Towards Efficient Point Cloud Graph Neural Networks Through Architectural Simplification"
  authors: Tailor et al.
  url: https://arxiv.org/abs/2108.06317
  pdf:
  #video: "https://youtu.be/l_Z6xgEBHBg"
  location: "Zoom meetings"
  summary: |
    This papers suggests a efficient method to learn point cloud data that reduces latency and memory consumption. 
    
    Point cloud architectures incorporate geometric priors in the first layer but as it goes deeper geometric priors are lost.
    
    So this paper suggests featrue extractor at first layer with several geometric priors. (target postion, source position, relative postion, euclidean distance)
    
    By using feature extractor and simplifed DGCNN, we could get good performance in less time and memory.


