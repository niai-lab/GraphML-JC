- presenter: Wooyeol Jung
  date: 9.14.2023
  paper: "Semi-Supervised Classification with Graph Convolutional Networks"
  authors: Kipf & Welling
  url: https://arxiv.org/abs/1609.02907
  location: "산학기술관 702-1"

- presenter: Hyungeun Lee
  date: 7.19.2023
  paper: "Variance-invariance-covariance regularization for self-supervised learning"
  authors: Bardes et al.
  url: https://arxiv.org/abs/2105.04906
  location: "산학기술관 702-1"
  
- presenter: Hyungeun Lee
  date: 7.19.2023
  paper: "Unsupervised learning of visual features by contrasting cluster assignments."
  authors: Caron et al.
  url: https://arxiv.org/abs/2006.09882
  location: "산학기술관 702-1"
  
- presenter: Seungwon Yoo
  date: 7.19.2023
  paper: "Emerging Properties in Self-Supervised Vision Transformers"
  authors: Caron et al.
  url: https://arxiv.org/abs/2104.14294
  location: "산학기술관 702-1"
  
- presenter: Seungwon Yoo
  date: 7.19.2023
  paper: "Exploring Simple Siamese Representation Learning"
  authors: Chen et al.
  url: https://arxiv.org/abs/2011.10566
  location: "산학기술관 702-1"
  
- presenter: Seungwon Yoo
  date: 7.19.2023
  paper: "Bootstrap your own latent: A new approach to self-supervised Learning"
  authors: Grill et al.
  url: https://arxiv.org/abs/2006.07733
  location: "산학기술관 702-1"
  
- presenter: Woong Hwang
  date: 7.19.2023
  paper: "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations"
  authors: Dwibedi et al.
  url: https://arxiv.org/abs/2104.14548
  location: "산학기술관 702-1"
  
- presenter: Woong Hwang
  date: 7.19.2023
  paper: "A Simple Framework for Contrastive Learning of Visual Representations"
  authors: Chen et al.
  url: https://arxiv.org/abs/2002.05709
  location: "산학기술관 702-1"

- presenter: Taehoon Park
  date: 10.15.2021
  paper: "Learning Neural Causal Models from Unknown Interventions"
  authors: Ke et al.
  url: https://arxiv.org/abs/1910.01075
  pdf: /GraphML-JC/pdf/Learning_Neural_Causal_Models_from_Unknown_Interventions.pdf
  #video: https://youtu.be/fCqoPeP5WRU
  location: "Zoom meetings"
  summary: >
   이번 논문은 system을 contional probability로 modeling 하는 경우에서 graph sturcture recovery에 관한 내용입니다. 
   사용된 graph의 structure는 DAG(Directed Acyclic Graph)입니다. 
   Modeling을 할 때 기존의 system에서 관측가능한 data인 observational data 뿐만 아니라, system에서 일어날 것 같지 않은 
   condition의 값을 할당한 data인 interventional data까지 training 단계에서 사용할 경우 sturcture recovery가 더 좋은 결과를 낸다는 것이 주요내용입니다. 
   또한 모든 실험에서 Intervention을 apply할 때 sparse하게 하나의 node에만 intervention을 사용했습니다. 
   Intervention을 사용할 경우 논문의 모든 dataset(synthetic/real)에서 best performance를 보였으며, 
   어떠한 node에서 intervention이 일어났는 지 모르는 경우, 이를 inference하는 방법도 제시하였으며 괜찮은 성능을 가집니다. 
   Intervention의 경우 data의 perturbation과 동일한 역할을 하는것으로 보여집니다.

- presenter: Donghee Kang
  date: 10.22.2021
  paper: "Contrastive Self-supervised Learning for Graph Classification"
  authors: Zeng et al.
  url: https://arxiv.org/abs/2009.05923
  pdf: /GraphML-JC/pdf/Contrastive_self-supervised_Learning_for_GC.pdf
  #video: https://youtu.be/-VD3UeHTqkU
  location: "Zoom meetings"
  summary: "contrastive self supervised learning 방법을 이용해 graph classification이 발생할 수 있는 overfitting을 예방하려는 방법을 제안하였습니다. augmentation된 그래프들이 같은 그래프에서 나왓는지 아닌지를 prediction하는 모델을 학습한 후 fine-tuning을 통해 classification을 수행하는 방법과 prediction과 classification을 동시에 수행 하며 regularization을 동시에 수행하는 두가지 방법이 있습니다. 두 방법 모두 단순히 classification만을 수행하는 모델보다 좋은 성능을 보였고 overfitting문제를 더 잘 해결 할 수 있었습니다."

- presenter: Hyunmok Park
  date: 10.29.2021
  paper: "Scaling Graph-based Deep Learning models to larger networks"
  authors: Ferriol-Galmés et al.
  url: https://arxiv.org/abs/2110.01261
  pdf:
  #video: https://youtu.be/vAbpM1Hp-vQ
  location: "Zoom meetings"
  summary: |
    Graph Neural Networks (GNN) have shown a strong potential to be integrated into commercial products for network control and management. However, the Graph Neural Networking challenge 2021 brings a practical limitation of existing GNN-based solutions for networking: the lack of generalization to larger networks. The goal of this challenge is to create a scalable Network Digital Twin (i.e., a network model) based on neural networks, which can accurately estimate QoS performance metrics given a network state snapshot. The proposed GNN-Based solution pursues two main objectives:
    
    (1) Finding a good representation for the network components supported by the model.
    
    (2) Exploit scale-independent features of networks.
    
    There are two main properties for networks become larger: (i) higher link capacities and (ii) longer paths. By defining link capacities as the product of a virtual reference link capacity and a scale factor, GNN can learn to make accurate estimates on any combination of these two features. By defining Lmax as a configurable parameter and split flows exceeding Lmax into different queue-link sequences, the model can independently learn each representation. The model was trained on graphs size of |V| = 25 ~ 300 and tested with graphs size of |V| = 50 ~ 300. To generate a wide set of topologies of variable size, all graph topologies have been artificially generate using Power-Law Out-Degree Algorithm. By using grid search data augmentation, the model was trained to cover a broad combination of virtual reference link capacity and a scale factor. As a result, the model obtains better accuracy in topologies seen during the training phase (50 to 99 nodes), achieving an average error of 4.5%. As the topology size increases, the average error stabilizes to ≈10%.

    This paper have presented a novel GNN-based model that addresses a main limitation of existing ML-based models applied to networks: generalizing accurately to considerably larger networks.
  
- presenter: Juhyeon Kim
  date: 11.05.2021
  paper: "Connecting the dots: Multivariate time series forecasting with graph neural networks"
  authors: Wu et al.
  url: https://arxiv.org/abs/2005.11650
  pdf:
  #video: https://youtu.be/q253JT_R8aI
  location: "Zoom meetings"
  summary: |
    Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields. 
    Until now, modeling multivariate time series forecasting they assume that each variable depends on its historical values. 
    But in real world data each variable depends not only on its historical values but also on other variables. And existing methods fail to fully exploit latent spatial dependencies between pairs of variables.
    This paper propose “MTGNN” that can simultaneously learn the graph structure and the GNN for time series in an end-to-end framework.
    The Main frameworks of “MTGNN” are

    1.	Graph Learning Layer
    -	Extract sparse graph adjacency matrix

    2.	Graph Convolution module
    -	Address spatial dependencies among Variables
    
    3.	Temporal Convolution module
    -	Capture temporal patterns
    
    MTGNN achieves great performance on two traffic dataset and show that learning dependencies between variables are more capable of indicating extreme traffic conditions of the central node in advance.

- presenter: ""
  date: 11.12.2021
  paper: "AISTATS Review"
  authors: "None"
  url: "None"
  pdf:
  video: "None"
  location: ""
  summary:
  
- presenter: Haesung Pyeon
  date: 11.19.2021
  paper: "Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks"
  authors: Csordás et al.
  url: https://arxiv.org/abs/2010.02066
  pdf:
  #video: "https://youtu.be/gPpifJ5ulVs"
  location: "Zoom meetings"
  summary: |
    Neural Networks whose subnetworks implement reusable functions are expected to offer numerous advantages. Modularity provides a way of achieving compositionality which is essential for systemic generalization. 
    This paper defines functional modules whether modules implementing specific functionality emerge. 
    By training probabilistic, binary but differentiable masks for all weight, the result shows the module necessary to perform target function. 
    This paper investigates whether NN uses different modules for very different functions and reuses the same modules for the same functions. 
    Many typical NNs are good at specializing modules well but not reusing modules for identical functions. FNN and LSTM share weights depending more on the location of the inputs/outputs than similarity of the performed operations. 
    CNN mostly depend on class-exclusive, non-shared weights in feature detection. Reusing property does not emerge naturally and the same functionality is re-learned (redundant and potentially harmful). 
    There are several potential explanations about poor generalization: NN find it hard to learn to represent data similarly along different information routes that can in principle be processed by a single module. 
    NN might not have learned the correct algorithm to solve some problems. 

- presenter: Taehoon Park
  date: 11.26.2021
  paper: "Perturbation theory approach to study the latent space degeneracy of Variational Autoencoders"
  authors: Andrés-Terré & Lió
  url: https://arxiv.org/abs/1907.05267
  pdf:
  #video: "https://youtu.be/jd60ajpFUmg"
  location: "Zoom meetings"
  summary: |
    VAEs provide a lower dimensional representation of the data, in the form of a set of generative functions capable to reproduce and reconstruct the original input.
    Inspired by perturbation analysis in quantum physics, we have developed an approach to unveil structures and the energy spectrum encoded in the data by using the generative functions extracted from a VAE
    Applying a certain perturbed Hamiltonian over the generated function.

    For implementation, encoder returns the function that returns unique solution for the system. To generate different energy level, perturbation approach linearly combines unperturbed energy and Hamiltonian.

    The embeddings generated from the fully trained VAE provide a space where the clusters are clearly seperable. This work is very beginning of the line of study, and further studies will promising for understanding dynamic system with new approach.

- presenter: Donghee Kang
  date: 12.03.2021
  paper: "Towards Efficient Point Cloud Graph Neural Netwokrs Through Architectural Simplification"
  authors: Tailor et al.
  url: https://arxiv.org/abs/2108.06317
  pdf:
  #video: "https://youtu.be/l_Z6xgEBHBg"
  location: "Zoom meetings"
  summary: |
    This papers suggests a efficient method to learn point cloud data that reduces latency and memory consumption. 
    
    Point cloud architectures incorporate geometric priors in the first layer but as it goes deeper geometric priors are lost.
    
    So this paper suggests featrue extractor at first layer with several geometric priors. (target postion, source position, relative postion, euclidean distance)
    
    By using feature extractor and simplifed DGCNN, we could get good performance in less time and memory.


