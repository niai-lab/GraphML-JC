- presenter: Seungwon Yu
  date: 11.16.2023
  paper: "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition"
  authors: Yan et al. 
  url: https://arxiv.org/abs/1801.07455
  location: "산학기술관 702-1"
  summary: >

- presenter: HyunGeun Lee
  date: 11.09.2023
  paper: "Graph Element Networks: adaptive, structured computation and memory"
  authors: Alet et al. 
  url: https://arxiv.org/abs/1904.09019
  location: "산학기술관 702-1"
  summary: > 
    Graph Element Networks는 partial differential equation의 solver 중 하나인 Finite element method에서 아이디어를 얻은 모델로서, function은 locally linear하다는 inductive bias를 가지고 있습니다. 
    저자는 이러한 특성을 바탕으로 function to function mapping task에 GENs 모델을 적용하였습니다. 
    GENs 모델은 기존의 그래프의 데이터 포인트를 직접적으로 GNN의 node 혹은 edge entities에 상응시켜서 사용하는 것이 아닌 metric space 상에 정의된 data points를 무게중심의 일반화된 방법론과 softmax를 활용한 attention과 흡사한 방식을 통해 grid라는 일종의 template graph의 initial node representation으로 맵핑을 합니다.
    이러한 특성 덕분에 데이터가 그래프 set에 국한되는 것이 아니라 metric space 상에 자유롭게 위치할 수 있게 되고, template grid graph의 density를 조정함으로써 computaional cost 와 accuracy 사이의 trade-off에 있어 유연성을 가진다고 볼 수 있습니다. 
    이 논문에 대하여 한 가지 의문이 드는 점은 GENs의 output이 grid graph의 위치에 대한 미분이 가능하여 template graph의 topology를 조정할 수 있다는 부분입니다. 
    이러한 topology는 철저히 data-driven으로 학습이 되는 것이기 때문에 특정 영역으로 데이터 쏠림이 발생한다면 template graph의 노드 또한 그 쪽으로 모이는 현상이 발생하게 됩니다. 
    이렇게 되면 추후 extrapolation test 혹은 OOD generalization에 있어서 이 현상이 단점으로 부각될 수도 있지 않을까하는 우려가 있습니다.
    이에 대한 검증을 위한 추가 실험이 있었다면 더욱 좋았을 것 같습니다. 

- presenter: Wooyul Jung
  date: 11.02.2023
  paper: "Principal Neighbourhood Aggregation for Graph Nets"
  authors: Corso et al. 
  url: https://arxiv.org/abs/2004.05718
  location: "산학기술관 702-1"
  summary: > 
      위 논문은 'How powerful are Graph Neural Networks?'애 이어서 그래프의 isomorphism problem을 해결하고자 하는 시도입니다.
      discrete feature에서는 sum aggregator가 유용하다는 'How powerful are Graph Neural Networks?'의 주장과는 달리 본 논문은
      continuous feature에 대해서는 multiple aggregators가 필요하다고 말합니다.
      이 주장에 대한 근거로 여러 수학적 접근으로 엄밀성을 갖추었으며 real world bentchmarks에서 기존 방법론보다 훌륭한 결과를 보임을 밝힙니다. 

- presenter: Woong Hwang
  date: 10.12.2023
  paper: "How Powerful are Graph Neural Networks?"
  authors: Xu et al. 
  url: https://arxiv.org/abs/1810.00826
  location: "산학기술관 702-1"
  summary: > 
      저자들은 Graph Neural Network의 aggregation mechanism의 expressive power에 대한 연구를 진행하였습니다. 
      그리고 이를 Graph isomorphism test를 위한 1-WL(Weisfeiler-Lehman) test와 연결지어 설명하며,
      1-hop neighbourhood의 information을 aggregation하는 spatial GNN은 이 1-WL test보다 더 좋은 성능을 낼 수 없다는 것을 보입니다.
      또한, discrete한 feature를 가지는 그래프의 message passing과정을 tree형태로 inject하여 해석하면서 non-isomorphic 그래프 쌍의 aggregation에 대한 구분에 있어 실패하는 사례를 보여줍니다.
      저자들은 이러한 한계점을 극복하기 위해 Sum aggregation의 중요성을 언급하며, Expressive power를 극대화한 GIN 아키텍쳐를 제안하며 graph-level task에서 우수한 성능을 보여주었습니다. 
  

- presenter: HyunGeun Lee
  date: 10.05.2023
  paper: "Neural Message Passing for Quantum Chemistry"
  authors: Gilmer et al. 
  url: https://arxiv.org/abs/1704.01212
  location: "산학기술관 702-1"
  summary: > 
    본 논문은 기존의 다양한 GNN architectures를 하나의 framework로 통합했다는 것에 의미가 큰 논문입니다. 그 framework는 Message Passing Neural Network framework라고 부르며, 이는 총 3가지 단계, Message passing, Update, 그리고 Readout로 구성되어 있습니다.
    먼저, Message Passing 단계에서는 인접 노드의 정보를 계산하고 그 정보를 sum, mean, max, min 등의 연산을 통해 aggregation합니다. 이 때, aggregation된 정보를 message라고 합니다.
    다음으로, 업데이트 단계에서 업데이트 함수 $U_t$는 target node와 메시지의 hidden state를 입력으로 받아 target node의 hidden state를 업데이트합니다. 
    마지막으로, readout 단계에서는 전체 그래프에 대한 representation vector를 계산합니다.

    저자들은 이 Framework를 통해 MPNN Variant를 개발하여 QM9 데이터셋에 대하여 Chemical Accuracy를 달성하였으며, 이 결과에 대한 더 자세한 내용은 논문에 기술되어 있습니다.
    

- presenter: Seungwon Yu
  date: 9.21.2023
  paper: "Graph Attention Networks"
  authors: Veličković et al. 
  url: https://arxiv.org/abs/1710.10903
  location: "산학기술관 702-1"
  summary: > 
    Graph attention networks(GAT)는 2018년도 ICLR에서 발표된 논문입니다. GAT는 마스킹된 self-attention layer를 활용하여 기존의 spectral graph convolution의 단점을 해결했다고 합니다.
    인접한 노드에 정규화된 attention coefficient를 곱하여 합산하는 aggregation 방법론을 사용함으로써, 행렬 연산 및 그래프 구조에 의존하지 않는 방법론을 제안합니다. 
    GAT는 그래프의 구조에 영향을 받지 않기 때문에 기존의 transductive task 뿐만 아니라 inductive task에서도 좋은 성능을 보여준다고 합니다. 
    이를 검증하기 위해 저자는 transductive task 벤치마킹을 위해 Cora, Citeseer 그리고 Pubmed 데이터셋을 사용하였습니다. 
    또한, inductive task에 대한 벤치마킹을 위해 PPI(Protein-protein interaction) 데이터셋을 사용하여 GAT의 SOTA 성능을 검증하였습니다.
    이번 미팅을 통해 GAT에서 사용한 self-attention layer의 수식에 대해 더 깊은 이해를 하였고, GNN 도메인에서 중요하게 여겨지는 방법론에 대한 기초를 다지는 시간을 가졌습니다.

- presenter: Wooyul Jung
  date: 9.14.2023
  paper: "Semi-Supervised Classification with Graph Convolutional Networks"
  authors: Kipf & Welling
  url: https://arxiv.org/abs/1609.02907
  location: "산학기술관 702-1"
  summary: > 
    이번 논문은 spectral graph convolution의 first-order approximation으로 유도되는 graph convolutional network를 제안합니다.
    그동안의 graph based semi-supervised learning에서 graph의 smoothness만 고려한 것과는 달리 위 모델은 edge weight과 node간의 전달을 통해 고차원적인 정보를 다룰 수 있습니다.
    다양한 citation network datasets에서 실험한 결과 다른 모델들에 비해 모두 뛰어난 성능을 보였습니다. 
    발표에서는 수식의 이해를 돕기 위해 graph laplacian과 graph fourier transform에 대한 설명을 더하였으며 마지막에는 transformer와 GCN의 유사성을 다뤘습니다.

- presenter: Hyungeun Lee
  date: 7.19.2023
  paper: "Variance-invariance-covariance regularization for self-supervised learning"
  authors: Bardes et al.
  url: https://arxiv.org/abs/2105.04906
  location: "산학기술관 702-1"
  
- presenter: Hyungeun Lee
  date: 7.19.2023
  paper: "Unsupervised learning of visual features by contrasting cluster assignments."
  authors: Caron et al.
  url: https://arxiv.org/abs/2006.09882
  location: "산학기술관 702-1"
  
- presenter: Seungwon Yu
  date: 7.19.2023
  paper: "Emerging Properties in Self-Supervised Vision Transformers"
  authors: Caron et al.
  url: https://arxiv.org/abs/2104.14294
  location: "산학기술관 702-1"
  
- presenter: Seungwon Yu
  date: 7.19.2023
  paper: "Exploring Simple Siamese Representation Learning"
  authors: Chen et al.
  url: https://arxiv.org/abs/2011.10566
  location: "산학기술관 702-1"
  
- presenter: Seungwon Yu
  date: 7.19.2023
  paper: "Bootstrap your own latent: A new approach to self-supervised Learning"
  authors: Grill et al.
  url: https://arxiv.org/abs/2006.07733
  location: "산학기술관 702-1"
  
- presenter: Woong Hwang
  date: 7.19.2023
  paper: "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations"
  authors: Dwibedi et al.
  url: https://arxiv.org/abs/2104.14548
  location: "산학기술관 702-1"
  
- presenter: Woong Hwang
  date: 7.19.2023
  paper: "A Simple Framework for Contrastive Learning of Visual Representations"
  authors: Chen et al.
  url: https://arxiv.org/abs/2002.05709
  location: "산학기술관 702-1"

- presenter: Taehoon Park
  date: 10.15.2021
  paper: "Learning Neural Causal Models from Unknown Interventions"
  authors: Ke et al.
  url: https://arxiv.org/abs/1910.01075
  pdf: /GraphML-JC/pdf/Learning_Neural_Causal_Models_from_Unknown_Interventions.pdf
  #video: https://youtu.be/fCqoPeP5WRU
  location: "Zoom meetings"
  summary: >
   이번 논문은 system을 contional probability로 modeling 하는 경우에서 graph sturcture recovery에 관한 내용입니다. 
   사용된 graph의 structure는 DAG(Directed Acyclic Graph)입니다. 
   Modeling을 할 때 기존의 system에서 관측가능한 data인 observational data 뿐만 아니라, system에서 일어날 것 같지 않은 
   condition의 값을 할당한 data인 interventional data까지 training 단계에서 사용할 경우 sturcture recovery가 더 좋은 결과를 낸다는 것이 주요내용입니다. 
   또한 모든 실험에서 Intervention을 apply할 때 sparse하게 하나의 node에만 intervention을 사용했습니다. 
   Intervention을 사용할 경우 논문의 모든 dataset(synthetic/real)에서 best performance를 보였으며, 
   어떠한 node에서 intervention이 일어났는 지 모르는 경우, 이를 inference하는 방법도 제시하였으며 괜찮은 성능을 가집니다. 
   Intervention의 경우 data의 perturbation과 동일한 역할을 하는것으로 보여집니다.

- presenter: Donghee Kang
  date: 10.22.2021
  paper: "Contrastive Self-supervised Learning for Graph Classification"
  authors: Zeng et al.
  url: https://arxiv.org/abs/2009.05923
  pdf: /GraphML-JC/pdf/Contrastive_self-supervised_Learning_for_GC.pdf
  #video: https://youtu.be/-VD3UeHTqkU
  location: "Zoom meetings"
  summary: "contrastive self supervised learning 방법을 이용해 graph classification이 발생할 수 있는 overfitting을 예방하려는 방법을 제안하였습니다. augmentation된 그래프들이 같은 그래프에서 나왓는지 아닌지를 prediction하는 모델을 학습한 후 fine-tuning을 통해 classification을 수행하는 방법과 prediction과 classification을 동시에 수행 하며 regularization을 동시에 수행하는 두가지 방법이 있습니다. 두 방법 모두 단순히 classification만을 수행하는 모델보다 좋은 성능을 보였고 overfitting문제를 더 잘 해결 할 수 있었습니다."

- presenter: Hyunmok Park
  date: 10.29.2021
  paper: "Scaling Graph-based Deep Learning models to larger networks"
  authors: Ferriol-Galmés et al.
  url: https://arxiv.org/abs/2110.01261
  pdf:
  #video: https://youtu.be/vAbpM1Hp-vQ
  location: "Zoom meetings"
  summary: |
    Graph Neural Networks (GNN) have shown a strong potential to be integrated into commercial products for network control and management. However, the Graph Neural Networking challenge 2021 brings a practical limitation of existing GNN-based solutions for networking: the lack of generalization to larger networks. The goal of this challenge is to create a scalable Network Digital Twin (i.e., a network model) based on neural networks, which can accurately estimate QoS performance metrics given a network state snapshot. The proposed GNN-Based solution pursues two main objectives:
    
    (1) Finding a good representation for the network components supported by the model.
    
    (2) Exploit scale-independent features of networks.
    
    There are two main properties for networks become larger: (i) higher link capacities and (ii) longer paths. By defining link capacities as the product of a virtual reference link capacity and a scale factor, GNN can learn to make accurate estimates on any combination of these two features. By defining Lmax as a configurable parameter and split flows exceeding Lmax into different queue-link sequences, the model can independently learn each representation. The model was trained on graphs size of |V| = 25 ~ 300 and tested with graphs size of |V| = 50 ~ 300. To generate a wide set of topologies of variable size, all graph topologies have been artificially generate using Power-Law Out-Degree Algorithm. By using grid search data augmentation, the model was trained to cover a broad combination of virtual reference link capacity and a scale factor. As a result, the model obtains better accuracy in topologies seen during the training phase (50 to 99 nodes), achieving an average error of 4.5%. As the topology size increases, the average error stabilizes to ≈10%.

    This paper have presented a novel GNN-based model that addresses a main limitation of existing ML-based models applied to networks: generalizing accurately to considerably larger networks.
  
- presenter: Juhyeon Kim
  date: 11.05.2021
  paper: "Connecting the dots: Multivariate time series forecasting with graph neural networks"
  authors: Wu et al.
  url: https://arxiv.org/abs/2005.11650
  pdf:
  #video: https://youtu.be/q253JT_R8aI
  location: "Zoom meetings"
  summary: |
    Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields. 
    Until now, modeling multivariate time series forecasting they assume that each variable depends on its historical values. 
    But in real world data each variable depends not only on its historical values but also on other variables. And existing methods fail to fully exploit latent spatial dependencies between pairs of variables.
    This paper propose “MTGNN” that can simultaneously learn the graph structure and the GNN for time series in an end-to-end framework.
    The Main frameworks of “MTGNN” are

    1.	Graph Learning Layer
    -	Extract sparse graph adjacency matrix

    2.	Graph Convolution module
    -	Address spatial dependencies among Variables
    
    3.	Temporal Convolution module
    -	Capture temporal patterns
    
    MTGNN achieves great performance on two traffic dataset and show that learning dependencies between variables are more capable of indicating extreme traffic conditions of the central node in advance.

- presenter: ""
  date: 11.12.2021
  paper: "AISTATS Review"
  authors: "None"
  url: "None"
  pdf:
  video: "None"
  location: ""
  summary:
  
- presenter: Haesung Pyeon
  date: 11.19.2021
  paper: "Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks"
  authors: Csordás et al.
  url: https://arxiv.org/abs/2010.02066
  pdf:
  #video: "https://youtu.be/gPpifJ5ulVs"
  location: "Zoom meetings"
  summary: |
    Neural Networks whose subnetworks implement reusable functions are expected to offer numerous advantages. Modularity provides a way of achieving compositionality which is essential for systemic generalization. 
    This paper defines functional modules whether modules implementing specific functionality emerge. 
    By training probabilistic, binary but differentiable masks for all weight, the result shows the module necessary to perform target function. 
    This paper investigates whether NN uses different modules for very different functions and reuses the same modules for the same functions. 
    Many typical NNs are good at specializing modules well but not reusing modules for identical functions. FNN and LSTM share weights depending more on the location of the inputs/outputs than similarity of the performed operations. 
    CNN mostly depend on class-exclusive, non-shared weights in feature detection. Reusing property does not emerge naturally and the same functionality is re-learned (redundant and potentially harmful). 
    There are several potential explanations about poor generalization: NN find it hard to learn to represent data similarly along different information routes that can in principle be processed by a single module. 
    NN might not have learned the correct algorithm to solve some problems. 

- presenter: Taehoon Park
  date: 11.26.2021
  paper: "Perturbation theory approach to study the latent space degeneracy of Variational Autoencoders"
  authors: Andrés-Terré & Lió
  url: https://arxiv.org/abs/1907.05267
  pdf:
  #video: "https://youtu.be/jd60ajpFUmg"
  location: "Zoom meetings"
  summary: |
    VAEs provide a lower dimensional representation of the data, in the form of a set of generative functions capable to reproduce and reconstruct the original input.
    Inspired by perturbation analysis in quantum physics, we have developed an approach to unveil structures and the energy spectrum encoded in the data by using the generative functions extracted from a VAE
    Applying a certain perturbed Hamiltonian over the generated function.

    For implementation, encoder returns the function that returns unique solution for the system. To generate different energy level, perturbation approach linearly combines unperturbed energy and Hamiltonian.

    The embeddings generated from the fully trained VAE provide a space where the clusters are clearly seperable. This work is very beginning of the line of study, and further studies will promising for understanding dynamic system with new approach.

- presenter: Donghee Kang
  date: 12.03.2021
  paper: "Towards Efficient Point Cloud Graph Neural Netwokrs Through Architectural Simplification"
  authors: Tailor et al.
  url: https://arxiv.org/abs/2108.06317
  pdf:
  #video: "https://youtu.be/l_Z6xgEBHBg"
  location: "Zoom meetings"
  summary: |
    This papers suggests a efficient method to learn point cloud data that reduces latency and memory consumption. 
    
    Point cloud architectures incorporate geometric priors in the first layer but as it goes deeper geometric priors are lost.
    
    So this paper suggests featrue extractor at first layer with several geometric priors. (target postion, source position, relative postion, euclidean distance)
    
    By using feature extractor and simplifed DGCNN, we could get good performance in less time and memory.


